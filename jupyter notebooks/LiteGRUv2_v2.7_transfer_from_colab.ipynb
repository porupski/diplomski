{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72196bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(r\"C:\\Users\\Mr Cab Driver\\Documents\\aCreativeHub\\Projekti\\Speech-to-Text\\LiteGRUv1\\anaconda\\LiteGRUv2_v2_7\")\n",
    "# !dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0f058c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da50465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor: 11th Gen Intel(R) Core(TM) i5-11600 @ 2.80GHz\n",
      "Architecture: X86_64\n",
      "Cores: 12\n"
     ]
    }
   ],
   "source": [
    "import cpuinfo\n",
    "\n",
    "info = cpuinfo.get_cpu_info()\n",
    "\n",
    "print(\"Processor:\", info[\"brand_raw\"])\n",
    "print(\"Architecture:\", info[\"arch\"])\n",
    "print(\"Cores:\", info[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0c9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: GeForce GTX 760\n",
      "Device Type: 4\n",
      "Vendor: NVIDIA Corporation\n",
      "OpenCL Version: OpenCL 1.2 CUDA\n",
      "Global Memory: 4.00 GB\n",
      "Max Compute Units: 6\n"
     ]
    }
   ],
   "source": [
    "import pyopencl as cl\n",
    "\n",
    "def get_gpu_info():\n",
    "    platforms = cl.get_platforms()\n",
    "    platform = platforms[0]  # Assuming you have at least one platform\n",
    "    devices = platform.get_devices(device_type=cl.device_type.GPU)\n",
    "\n",
    "    if devices:\n",
    "        gpu = devices[0]  # Assuming you have at least one GPU\n",
    "        return f\"GPU Name: {gpu.name}\\nDevice Type: {gpu.type}\\nVendor: {gpu.vendor}\\nOpenCL Version: {gpu.version}\\nGlobal Memory: {gpu.global_mem_size / (1024 ** 3):.2f} GB\\nMax Compute Units: {gpu.max_compute_units}\"\n",
    "    else:\n",
    "        return \"No GPUs found.\"\n",
    "\n",
    "gpu_info = get_gpu_info()\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e29ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting char_to_idx.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile char_to_idx.py\n",
    "\n",
    "# Define the char_to_idx mapping\n",
    "char_to_idx = {\n",
    "    \" \": 0, \"[PAD]\": 1, \"[UNK]\": 2, \"a\": 3, \"b\": 4, \"c\": 5, \"d\": 6, \"e\": 7, \"f\": 8,\n",
    "    \"g\": 9, \"h\": 10, \"i\": 11, \"j\": 12, \"k\": 13, \"l\": 14, \"m\": 15, \"n\": 16, \"o\": 17,\n",
    "    \"p\": 18, \"q\": 19, \"r\": 20, \"s\": 21, \"t\": 22, \"u\": 23, \"v\": 24, \"w\": 25, \"x\": 26,\n",
    "    \"y\": 27, \"z\": 28, \"ä\": 29, \"ü\": 30, \"ć\": 31, \"č\": 32, \"đ\": 33, \"š\": 34, \"ž\": 35,\n",
    "    \"ӧ\": 36, \"1\": 37, \"2\": 38, \"3\": 39, \"4\": 40, \"5\": 41, \"6\": 42, \"7\": 43, \"8\": 44,\n",
    "    \"9\": 45, \"0\": 46, \".\": 47\n",
    "}\n",
    "\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a06562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "    \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "    Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "    \"\"\"\n",
    "    # Create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "    # Create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # Save the model state_dict()\n",
    "    print(f\"[INFO] Saving model state_dict to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## FONETSKA FREKVENCIJA U TEKSTU ##\n",
    "\n",
    "# import re\n",
    "# import json\n",
    "# import csv\n",
    "\n",
    "# def load_vocab(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         vocab = json.load(f)\n",
    "#     return vocab\n",
    "\n",
    "# def count_phonemes(text, vocab):\n",
    "#     phoneme_counts = {phoneme: 0 for phoneme in vocab}\n",
    "\n",
    "#     # Split the text into words and process each word\n",
    "#     words = re.findall(r'\\b\\w+\\b', text)\n",
    "#     for word in words:\n",
    "#         for phoneme in vocab:\n",
    "#             if phoneme in word:\n",
    "#                 phoneme_counts[phoneme] += 1\n",
    "\n",
    "#     return phoneme_counts\n",
    "\n",
    "# def calculate_average_percent(phoneme_counts):\n",
    "#     total = sum(phoneme_counts.values())\n",
    "#     average_percentages = {phoneme: count / total * 100 for phoneme, count in phoneme_counts.items()}\n",
    "#     return average_percentages\n",
    "\n",
    "# def write_csv(phoneme_counts, average_percentages, csv_file_path):\n",
    "#     with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#         writer = csv.writer(csvfile)\n",
    "#         writer.writerow(['Phoneme', 'Count', 'Average %'])\n",
    "#         for phoneme, count in phoneme_counts.items():\n",
    "#             average_percent = average_percentages[phoneme]\n",
    "#             writer.writerow([phoneme, count, average_percent])\n",
    "\n",
    "# def main():\n",
    "#     text_file_path = \"/content/Recenice_set_B.txt\"\n",
    "#     vocab_file_path = \"/content/vocab.json\"\n",
    "#     csv_file_path = \"/content/\"+\"phoneme_freq_text_B\"+\".csv\"\n",
    "\n",
    "#     with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "#         text = f.read()\n",
    "\n",
    "#     vocab = load_vocab(vocab_file_path)\n",
    "#     phoneme_counts = count_phonemes(text, vocab)\n",
    "#     average_percentages = calculate_average_percent(phoneme_counts)\n",
    "\n",
    "#     write_csv(phoneme_counts, average_percentages, csv_file_path)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "\n",
    "# print(f\".cvs FILE HAS BEEN SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c5587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing plot_loss_curves.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_loss_curves.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "\n",
    "        results is a .pkl file\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = results['train_loss']\n",
    "    train_acc = results['train_acc']\n",
    "    test_loss = results['test_loss']\n",
    "    test_acc = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Calculate LOSS trendlines using NumPy's polyfit\n",
    "    train_trend = np.polyfit(epochs, train_loss, 3)\n",
    "    test_trend = np.polyfit(epochs, test_loss, 4)\n",
    "\n",
    "    # Calculate ACCURACY trendlines using NumPy's polyfit\n",
    "    train_trend_acc = np.polyfit(epochs, train_acc, 3)\n",
    "    test_trend_acc = np.polyfit(epochs, test_acc, 4)\n",
    "\n",
    "    # Generate more points for smoother curves\n",
    "    epochs_extended = np.linspace(1, epochs, 100)\n",
    "\n",
    "    # Function to apply moving average smoothing\n",
    "    def moving_average(data, window_size):\n",
    "      return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    # Set the window size for moving average smoothing (choose an appropriate value)\n",
    "    window_size = 1\n",
    "\n",
    "    # Apply moving average smoothing to the LOSS data\n",
    "    smoothed_train_loss = moving_average(train_loss, window_size)\n",
    "    smoothed_test_loss = moving_average(test_loss, window_size)\n",
    "\n",
    "    # Apply moving average smoothing to the ACCURACY data\n",
    "    smoothed_train_acc = moving_average(train_acc, window_size)\n",
    "    smoothed_test_acc = moving_average(test_acc, window_size)\n",
    "\n",
    "    # Adjust the epochs array to match the length of the smoothed data\n",
    "    smoothed_epochs = epochs[window_size-1:]\n",
    "\n",
    "    # Plot smoothed LOSS\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(smoothed_epochs, smoothed_train_loss, label='train_loss')        # no smoothing as window_size = 1!!!\n",
    "    plt.plot(smoothed_epochs, smoothed_test_loss, label='test_loss')    # no smoothing as window_size = 1!!!\n",
    "    plt.title('Smoothed Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    #plt.ylim(0.5, 1.5)\n",
    "    plt.legend()\n",
    "\n",
    "    # # Plot LOSS with trendlines\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # plt.plot(epochs, loss, label='train_loss')\n",
    "    # plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.plot(epochs, np.polyval(train_trend, epochs), '--', label='train_trend')\n",
    "    plt.plot(epochs, np.polyval(test_trend, epochs), '--', label='test_trend')\n",
    "    plt.title('Loss trend')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    # Plot ACCURACY\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, smoothed_train_acc, label='train_accuracy')\n",
    "    plt.plot(epochs, smoothed_test_acc, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot ACCURACY with trendlines\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, np.polyval(train_trend_acc, epochs), '--', label='train_trend_acc')\n",
    "    plt.plot(epochs, np.polyval(test_trend_acc, epochs), '--', label='test_trend_acc')\n",
    "    plt.title('Accuracy trend')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    plt.show\n",
    "\n",
    "if \"__name__\" == \"__main__\":\n",
    "  plot_loss_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921fc4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_setup.py\n",
    "## DATASET CLASS ##\n",
    "\n",
    "import string\n",
    "import torch\n",
    "import torchaudio\n",
    "import subprocess\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio.transforms as T\n",
    "from char_to_idx import char_to_idx\n",
    "\n",
    "\n",
    "\n",
    "# ## DEFINIRANJE ##\n",
    "# ## MFCC FEATURE EXTRACTORA ##\n",
    "\n",
    "# # Oduzima se prvi koeficijent\n",
    "# n_mfcc = 13\n",
    "# fs = 16000 #Hz makni ovo kasnije\n",
    "\n",
    "# # Making a class for MFCC that does MFCC then drops the 0th coeff\n",
    "\n",
    "\n",
    "class MFCCWithoutZeroth(T.MFCC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MFCCWithoutZeroth, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        mfcc_data = super(MFCCWithoutZeroth, self).__call__(waveform)\n",
    "        mfcc_data = mfcc_data[:, 1:]\n",
    "        return mfcc_data\n",
    "\n",
    "\n",
    "# # Apply the custom transform\n",
    "# #mfcc_data = mfcc_transform(waveform)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for\n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "#from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str,\n",
    "    train_tran: str,\n",
    "    test_dir: str,\n",
    "    test_tran: str,\n",
    "    mfcc_transform: MFCCWithoutZeroth,\n",
    "    batch_size: int,\n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "  \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "  Takes in a training directory and testing directory path and turns\n",
    "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    mfcc_transform: MFCC transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader).\n",
    "\n",
    "    Example usage:\n",
    "      train_dataloader, test_dataloader, = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                             test_dir=path/to/test_dir,\n",
    "                             mfcc_transform=some_transform,\n",
    "                             batch_size=32,\n",
    "                             num_workers=4)\n",
    "  \"\"\"\n",
    "\n",
    "  # Load audio transcriptions\n",
    "  #with open(train_tran, 'r') as file:\n",
    "  #    train_audio_transcriptions = [line.strip() for line in file]\n",
    "  #with open(test_tran, 'r') as file:\n",
    "  #    test_audio_transcriptions = [line.strip() for line in file]\n",
    "\n",
    "\n",
    "  # Turn Audio sets into Dataloaders\n",
    "\n",
    "  train_data = ASRDataset(train_dir,\n",
    "                          train_tran,\n",
    "                          mfcc_transform,\n",
    "                          char_to_idx)\n",
    "\n",
    "  train_dataloader = DataLoader(train_data,\n",
    "                                batch_size=batch_size,\n",
    "                                collate_fn=collate_fn,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=True,)\n",
    "\n",
    "  test_data = ASRDataset(test_dir,\n",
    "                         test_tran,\n",
    "                         mfcc_transform,\n",
    "                         char_to_idx)\n",
    "\n",
    "  test_dataloader = DataLoader(test_data,\n",
    "                               batch_size=batch_size,\n",
    "                               collate_fn=collate_fn,\n",
    "                               shuffle=False,\n",
    "                               num_workers=num_workers,\n",
    "                               pin_memory=True,)\n",
    "\n",
    "  return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## DATASET 4 ##\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, folder_path, audio_transcriptions_path, mfcc_transform, char_to_idx):\n",
    "        self.folder_path = folder_path\n",
    "        self.mfcc_transform = mfcc_transform\n",
    "        self.char_to_idx = char_to_idx\n",
    "        #self.audio_transcriptions_path = audio_transcriptions_path\n",
    "        #self.wav_audio = None  # Initialize with None\n",
    "        #self.audio_cache = {}  # Cache dictionary\n",
    "\n",
    "        audio_filenames = sorted(os.listdir(self.folder_path))\n",
    "        self.audio_file_paths = [os.path.join(self.folder_path, filename) for filename in sorted(audio_filenames)]\n",
    "\n",
    "\n",
    "        # # Load audio filenames and transcriptions into memory\n",
    "        # with open(audio_filenames_path, 'r') as file:\n",
    "        #     self.audio_filenames = [line.strip() for line in file]\n",
    "\n",
    "        with open(audio_transcriptions_path, 'r', encoding=\"utf-8\") as file:\n",
    "            self.audio_transcriptions = [line.strip() for line in file]\n",
    "\n",
    "        # Check if the lengths match\n",
    "        # assert len(self.audio_filenames) == len(self.audio_transcriptions), \"Filenames and transcriptions do not match\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_transcriptions)\n",
    "\n",
    "\n",
    "    def preprocess_transcription(self, trans):\n",
    "        # Convert to lowercase\n",
    "        trans = trans.lower()\n",
    "\n",
    "        #print(f\"class after trans: {trans}\")\n",
    "\n",
    "        # Remove punctuation marks\n",
    "\n",
    "        #trans = ' '.join('[UNK]' if word in set_of_special_symbols else word for word in trans.split())\n",
    "        #trans = ' '.join('[UNK]' if word in string.punctuation else word for word in trans.split())\n",
    "        #trans = ' '.join(char_to_idx.get(word, '[UNK]') for word in trans.split())\n",
    "\n",
    "        trans = ''.join(char for char in trans if char not in string.punctuation)\n",
    "\n",
    "\n",
    "\n",
    "        #unk_index = char_to_idx.get('[UNK]', -1)  # Get the index for the [UNK] token, default to -1 if not found\n",
    "        #trans = ' '.join(str(char_to_idx.get(word, unk_index)) for word in trans.split())\n",
    "        #print(f\"class before trans: {trans}\")\n",
    "\n",
    "\n",
    "        #trans = ''.join(char for char in trans if char not in string.punctuation + '“”’‘—-')\n",
    "        #print(trans)\n",
    "\n",
    "        # Process other special symbols if needed\n",
    "\n",
    "        return trans\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the corresponding transcription\n",
    "        audio_transcription = self.audio_transcriptions[idx]\n",
    "\n",
    "\n",
    "        # Construct the full audio file path based on the index\n",
    "        audio_filename = sorted(os.listdir(self.folder_path))[idx]\n",
    "        audio_file_path = os.path.join(self.folder_path, audio_filename)\n",
    "\n",
    "        # Load audio using torchaudio\n",
    "        mp3_audio, orig_freq = torchaudio.load(audio_file_path)\n",
    "\n",
    "        # Resample audio to 16000 Hz\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=orig_freq, new_freq=16000)\n",
    "        wav_audio = resampler(mp3_audio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #self.wav_audio = wav_audio\n",
    "        #print(f\"class self.wav_audio: {wav_audio}\")\n",
    "\n",
    "        # Apply MFCC transformation\n",
    "        mfcc = self.mfcc_transform(wav_audio).permute(0, 2, 1)\n",
    "\n",
    "        # Convert transcription to target labels using char_to_idx\n",
    "        processed_transcription = self.preprocess_transcription(audio_transcription)\n",
    "        target_labels = [self.char_to_idx[c] if c in self.char_to_idx else self.char_to_idx[\"[UNK]\"] for c in processed_transcription]\n",
    "\n",
    "        return mfcc, target_labels\n",
    "\n",
    "\n",
    "\n",
    "## PRETVARANJE MFCC U 1D +PAD ##\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unzip the batch into a list of MFCC sequences and a list of transcriptions\n",
    "    mfccs, transcriptions = zip(*batch)\n",
    "\n",
    "\n",
    "    # print(f\"col transcriptions: {transcriptions}\")\n",
    "\n",
    "    # Find the maximum sequence length along both dimensions\n",
    "    max_time_steps = max(mfcc.shape[1] for mfcc in mfccs)\n",
    "    #num_mfcc_coeffs = mfccs[0].shape[2]  # Assuming all tensors have the same number of MFCC coefficients\n",
    "\n",
    "    #print(f\"col max length here: {max_time_steps}\")\n",
    "    #print(f\"col mfcc coeffs: {num_mfcc_coeffs}\")\n",
    "\n",
    "    # for item in mfccs:\n",
    "    #     print(f\"col item.shape {item.shape}\")\n",
    "\n",
    "    # Pad the transposed MFCC sequences to have the same length\n",
    "    mfccs = [torch.nn.functional.pad(mfcc, (0, 0, 0, max_time_steps - mfcc.shape[1])) for mfcc in mfccs]\n",
    "    #print(f\"col mfccs padded len {len(mfccs)}\")\n",
    "\n",
    "    # for item in mfccs:\n",
    "    #     print(f\"col item.shape padded {item.shape}\")\n",
    "\n",
    "    # Stack the padded MFCC sequences into a batched tensor\n",
    "    mfccs = torch.stack(mfccs)\n",
    "\n",
    "    # Calculate the maximum length of transcriptions in the batch\n",
    "    max_transcription_length = max(len(trans) for trans in transcriptions)\n",
    "    # print(f\"col max_transcription_length {max_transcription_length}\")\n",
    "\n",
    "    # Pad the transcriptions to have the same length using [PAD] index\n",
    "    padded_transcriptions = [trans + [char_to_idx['[PAD]']] * (max_transcription_length - len(trans)) for trans in transcriptions]\n",
    "    targets = torch.tensor(padded_transcriptions)\n",
    "\n",
    "\n",
    "    # These targets are unpadded originals!!!!\n",
    "    # targets = [torch.tensor(target) for mfcc, target in batch]\n",
    "\n",
    "    # Return the batched MFCC sequences and transcriptions\n",
    "    return mfccs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336e135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_builder.py\n",
    "## DEFINIRANJE MODELA ##\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LiteGRUv2(nn.Module):\n",
    "\n",
    "  \"\"\"Creates a Lightweight LiteGRUv2 model for ASR.\n",
    "\n",
    "  Args:\n",
    "  num_classes: An integer indicating number of classes\n",
    "  in_channels: An integer indicating number of input features! # Number of MFCC coefficients, should be dynamic tho\n",
    "  hidden_units: An integer indicating number of hidden units between layers.\n",
    "  dropout: A float indicating the dropout rate (0.1 or lower is ok 12.8.2023.).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_classes, in_channels, hidden_units, dropout):\n",
    "      super(LiteGRUv2, self).__init__()\n",
    "\n",
    "      self.conv1d = nn.Conv1d(in_channels=in_channels, out_channels=hidden_units, kernel_size=5, padding=2)  # Equivalent to padding='same'\n",
    "      self.batchnorm1 = nn.BatchNorm1d(hidden_units)\n",
    "      self.birnn = nn.GRU(input_size=hidden_units, hidden_size=hidden_units, bidirectional=True, batch_first=True)\n",
    "      self.dropout = nn.Dropout(p=dropout)\n",
    "      self.batchnorm2 = nn.BatchNorm1d(hidden_units * 2)  # 128 (hidden size) * 2 (bidirectional)\n",
    "      self.fc = nn.Linear(hidden_units * 2, num_classes)\n",
    "      #self.softmax = nn.Softmax(dim=2)  # Apply softmax over the time dimension\n",
    "\n",
    "  def forward(self, x):\n",
    "      # print(f\"fw x.shape entering: {x.shape}\")\n",
    "      x = self.conv1d(x)\n",
    "      # print(f\"fw conv1d.shape: {x.shape}\")\n",
    "      x = self.batchnorm1(x)\n",
    "      # print(f\"fw batchnorm1.shape: {x.shape}\")\n",
    "      x = x.permute(0, 2, 1) # Expected shape for GRU input: (batch_size, sequence_length, input_size)\n",
    "      x, _ = self.birnn(x)\n",
    "      # print(f\"fw birnn(GRU).shape: {x.shape}\")\n",
    "      x = self.dropout(x)\n",
    "      # print(f\"fw dropout.shape: {x.shape}\")\n",
    "      x = x.permute(0, 2, 1)\n",
    "      x = self.batchnorm2(x)\n",
    "      # print(f\"fw batchnorm2.shape: {x.shape}\")\n",
    "      x = x.permute(0, 2, 1)\n",
    "      x = self.fc(x)\n",
    "      # print(f\"fw fc.shape: {x.shape}\")\n",
    "      #x = self.softmax(x)\n",
    "      # print(f\"fw softmax.shape: {x.shape}\")\n",
    "      return x\n",
    "\n",
    "# Create an instance of the LiteGRUv2\n",
    "# dropout = 0.1 # ne flukturira ko 0.2!\n",
    "# num_classes = 30  # Change this to your number of classes\n",
    "# in_channels = 12  # Number of MFCC coefficients, should be dynamic tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c98781c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile engine.py\n",
    "\n",
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "\n",
    "#import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from char_to_idx import idx_to_char\n",
    "\n",
    "#from train import HIDDEN_UNITS\n",
    "#from train import flush_cuda\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# print every which freq:\n",
    "freq = 20\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ctc_accuracy(logits, target_lengths, transcriptions, device):\n",
    "\n",
    "    # print(f\"ctcacc logits.shape: {logits.shape}\")\n",
    "    # print(f\"ctcacc target_lengths.shape: {target_lengths.shape}\")\n",
    "    # print(f\"ctcacc target_lengths: {target_lengths}\")\n",
    "    # print(f\"ctcacc transcriptions.shape: {transcriptions.shape}\")\n",
    "    # print(f\"ctcacc transcriptions: {transcriptions}\")\n",
    "\n",
    "    # Convert logits to predicted labels using argmax\n",
    "    predicted_labels = torch.argmax(logits, dim=2)\n",
    "\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    # print(f\"ctcacc predicted labels from logits: {predicted_labels}\")\n",
    "    # print(f\"ctcacc predicted labels length: {len(predicted_labels)}\")\n",
    "    # print(f\"ctcacc predicted labels shape: {predicted_labels.shape}\")\n",
    "\n",
    "\n",
    "    for i in range(predicted_labels.shape[1]):\n",
    "\n",
    "\n",
    "        predicted_seq = predicted_labels[:target_lengths[i], i].to(device)  # Trim padding from predicted sequence\n",
    "        reference_seq = transcriptions[i][:target_lengths[i]].to(device) # Trim padding from reference sequence\n",
    "\n",
    "        total_chars += len(reference_seq)\n",
    "        correct_chars += torch.sum(predicted_seq == reference_seq).item()\n",
    "\n",
    "\n",
    "        accuracy = correct_chars / total_chars\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               train_dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               num_classes: int,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    total_train_batches = len(train_dataloader)\n",
    "\n",
    "    for batch_idx, (batch_mfccs, batch_targets) in enumerate(train_dataloader):\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\"Working on the first batch...\")\n",
    "        elif batch_idx == total_train_batches // 2:\n",
    "            print(\"Working on the middle batch...\")\n",
    "        elif batch_idx == total_train_batches - 1:\n",
    "            print(\"Working on the last batch...\")\n",
    "\n",
    "\n",
    "        batch_mfccs = batch_mfccs.squeeze(1).permute(0, 2, 1).to(device)\n",
    "        # print(f\"tr batch_mfccs.shape: {batch_mfccs.shape}, batch_mfccs: {batch_mfccs}\")\n",
    "\n",
    "        batch_targets = [target.to(device) for target in batch_targets]\n",
    "        # print(f\"tr batch_targets: {batch_targets}\")\n",
    "\n",
    "        target_lengths = torch.tensor([len(target) - (target == 1).sum().item() for target in batch_targets], dtype=torch.long)\n",
    "        # print(f\"tr target_lengths: {target_lengths}\")\n",
    "        # print(f\"tr target_lengths.size: {target_lengths.size}\")\n",
    "\n",
    "\n",
    "        max_sequence_length = max(target_lengths)\n",
    "        # print(f\"tr max_sequence_length: {max_sequence_length}\")\n",
    "\n",
    "\n",
    "\n",
    "        outputs = model(batch_mfccs) ########################################################################## UPALI ME #######################################\n",
    "        #outputs = torch.randn(batch_mfccs.size(0), batch_mfccs.size(2), num_classes).to(device) ####### I OVO UGASITI ONDA\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"tr outputs.shape: {outputs.shape}, outputs: {outputs}\")\n",
    "        outputs = torch.permute(outputs, (1, 0, 2))\n",
    "        # print(f\"tr batch_mfccs.size(2): {batch_mfccs.size(2)}\")\n",
    "        # print(f\"tr outputs.premuted.shape: {outputs.shape} || CTCLoss expects: [T max time length, N batch_size, C num_classes]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        outputs = outputs.view(batch_mfccs.size(2), len(batch_mfccs), num_classes)\n",
    "        #outputs = outputs.reshape(batch_mfccs.size(0), max_sequence_length, num_classes)\n",
    "        #print(f\"tr outputs shape view: {outputs.shape}\")\n",
    "\n",
    "        #outputs = F.log_softmax(outputs, dim=2)\n",
    "\n",
    "        input_lengths = torch.tensor([outputs.shape[0] for _ in batch_mfccs])\n",
    "        #input_lengths_list = input_lengths.tolist()\n",
    "\n",
    "        # print(f\"tr input_lengths: {input_lengths}\")\n",
    "        # print(f\"tr input_lengths.shape: {input_lengths.shape}\")\n",
    "        # print(f\"tr input_lengths_list: {input_lengths_list}\")\n",
    "\n",
    "\n",
    "        # Initialize an empty tensor to hold the padded targets\n",
    "        targets = torch.zeros(len(batch_targets), max_sequence_length, dtype=torch.long)\n",
    "        # print(f\"tr targets raw: {targets}\")\n",
    "        # print(f\"tr targets raw.shape: {targets.shape}\")\n",
    "\n",
    "        target_lengths = torch.tensor([len(target) - (target == 1).sum().item() for target in batch_targets], dtype=torch.long)\n",
    "        # print(f\"tr target_lengths: {target_lengths}\")\n",
    "        # print(f\"tr target_lengths.shape: {target_lengths.shape}\")\n",
    "\n",
    "        for i, target in enumerate(batch_targets):\n",
    "            targets[i, :len(target)] = target\n",
    "\n",
    "        # print(f\"tr targets for batch_targets: {targets}\")\n",
    "        # print(f\"tr targets.shape for batch_targets: {targets.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Create a PackedSequence object\n",
    "        packed_sequence = rnn_utils.pack_padded_sequence(targets, target_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Access the depadded tensor and lengths\n",
    "        depadded_targets, depadded_lengths = rnn_utils.pad_packed_sequence(packed_sequence, batch_first=True)\n",
    "\n",
    "        # print(\"tr depadded_targets:\")\n",
    "        # print(depadded_targets)\n",
    "\n",
    "        # print(\"tr depadded lengths:\")\n",
    "        # print(depadded_lengths)\n",
    "\n",
    "\n",
    "\n",
    "        # depadded_targets = torch.zeros(len(batch_targets), max(target_lengths), dtype=torch.long)\n",
    "\n",
    "        # print(f\"tr depadded_targets: {depadded_targets}\")\n",
    "        # print(f\"tr depadded_targets.shape: {depadded_targets.shape}\")\n",
    "\n",
    "        # for i, target in enumerate(batch_targets):\n",
    "        #     depadded_targets[i, :target_lengths[i]] = target[target != 1]  # Exclude padding token (index 1)\n",
    "\n",
    "        # print(f\"tr depadded_targets for: {depadded_targets}\")\n",
    "        # print(f\"tr depadded_targets.shape for : {depadded_targets.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Train CTC accuracy in this batch\n",
    "        batch_acc = calculate_ctc_accuracy(outputs, target_lengths, depadded_targets, device)\n",
    "\n",
    "        # print(f\"tr targets: {targets}\")\n",
    "        # print(f\"tr targets.shape: {targets.shape}\")\n",
    "\n",
    "        # Loss function\n",
    "        loss = loss_fn(outputs, depadded_targets.float(), input_lengths, target_lengths)\n",
    "        # print(f\"tr loss: {loss}, batch: {batch_idx}\")\n",
    "\n",
    "        # Boiler plate (Zero the gradients, Backpropagation, Optimizer step)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # # Calculate gradient norms without accumulating gradients\n",
    "        # with torch.autograd.no_grad():\n",
    "        #     for name, param in model.named_parameters():\n",
    "        #         if param.grad is not None:\n",
    "        #             print(f\"tr Gradient norm for {name}: {torch.norm(param.grad).item()}\")\n",
    "\n",
    "\n",
    "        # Train total Loss and Acc\n",
    "        train_loss += loss.item()\n",
    "        train_acc += batch_acc\n",
    "\n",
    "    # Average loss and acc per batch\n",
    "    train_loss = train_loss / len(train_dataloader)\n",
    "    train_acc = train_acc / len(train_dataloader)\n",
    "    return train_loss, train_acc\n",
    "    #print(f\"total_train_loss: {total_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Save the model state_dict()\n",
    "\n",
    "\n",
    "    model_checkpoint = \"CHECKPOINT_LiteGRUv2.6_temp.pth\"\n",
    "\n",
    "    model_save_path = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/models/\" + model_checkpoint\n",
    "\n",
    "\n",
    "    torch.save(obj=model.state_dict(),\n",
    "               f=model_save_path)\n",
    "    print(f\"[SAVED] Saving CHECKPOINT model state_dict to: {model_save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              test_dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              num_classes: int,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Loop through TEST DataLoader batches\n",
    "\n",
    "        #test_iterator = tqdm(test_dataloader, desc='Testing', leave=False)\n",
    "\n",
    "\n",
    "        total_test_batches = len(test_dataloader)\n",
    "\n",
    "        for batch_idx, (batch_mfccs, batch_targets) in enumerate(test_dataloader):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            batch_mfccs = batch_mfccs.squeeze(1).permute(0, 2, 1).to(device)\n",
    "            # print(f\"te batch_mfccs.shape: {batch_mfccs.shape}, batch_mfccs: {batch_mfccs}\")\n",
    "\n",
    "            batch_targets = [target.to(device) for target in batch_targets]\n",
    "            # print(f\"te batch_targets: {batch_targets}\")\n",
    "\n",
    "            target_lengths = torch.tensor([len(target) - (target == 1).sum().item() for target in batch_targets], dtype=torch.long)\n",
    "            # print(f\"te target_lengths: {target_lengths}\")\n",
    "            # print(f\"te target_lengths.size: {target_lengths.size}\")\n",
    "\n",
    "            max_sequence_length = max(target_lengths)\n",
    "            # print(f\"te max_sequence_length: {max_sequence_length}\")\n",
    "\n",
    "\n",
    "            outputs = model(batch_mfccs) ########################################################################## UPALI ME #######################################\n",
    "            #outputs = torch.randn(batch_mfccs.size(0), batch_mfccs.size(2), num_classes).to(device) ####### I OVO UGASITI ONDA\n",
    "\n",
    "\n",
    "            # print(f\"tr outputs.shape: {outputs.shape}, outputs: {outputs}\")\n",
    "            outputs = torch.permute(outputs, (1, 0, 2))\n",
    "            # print(f\"te batch_mfccs.size(2): {batch_mfccs.size(2)}\")\n",
    "            # print(f\"te outputs.premuted.shape: {outputs.shape} || CTCLoss expects: [T max time length, N batch_size, C num_classes]\")\n",
    "\n",
    "            outputs = outputs.view(batch_mfccs.size(2), len(batch_mfccs), num_classes)\n",
    "            #outputs = outputs.reshape(batch_mfccs.size(0), max_sequence_length, num_classes)\n",
    "            #print(f\"te outputs shape view: {outputs.shape}\")\n",
    "\n",
    "            #outputs = F.log_softmax(outputs, dim=2)\n",
    "\n",
    "            input_lengths = torch.tensor([outputs.shape[0] for _ in batch_mfccs])\n",
    "            #input_lengths_list = input_lengths.tolist()\n",
    "\n",
    "            # print(f\"te input_lengths: {input_lengths}\")\n",
    "            # print(f\"te input_lengths.shape: {input_lengths.shape}\")\n",
    "            # print(f\"te input_lengths_list: {input_lengths_list}\")\n",
    "\n",
    "            # Initialize an empty tensor to hold the padded targets\n",
    "            targets = torch.zeros(len(batch_targets), max_sequence_length, dtype=torch.long)\n",
    "            # print(f\"te targets raw: {targets}\")\n",
    "            # print(f\"te targets raw.shape: {targets.shape}\")\n",
    "\n",
    "            target_lengths = torch.tensor([len(target) - (target == 1).sum().item() for target in batch_targets], dtype=torch.long)\n",
    "            # print(f\"te target_lengths: {target_lengths}\")\n",
    "            # print(f\"te target_lengths.shape: {target_lengths.shape}\")\n",
    "\n",
    "            for i, target in enumerate(batch_targets):\n",
    "                targets[i, :len(target)] = target\n",
    "\n",
    "            # print(f\"te targets for batch_targets: {targets}\")\n",
    "            # print(f\"te targets.shape for batch_targets: {targets.shape}\")\n",
    "\n",
    "            # Create a PackedSequence object\n",
    "            packed_sequence = rnn_utils.pack_padded_sequence(targets, target_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "            # Access the depadded tensor and lengths\n",
    "            depadded_targets, depadded_lengths = rnn_utils.pad_packed_sequence(packed_sequence, batch_first=True)\n",
    "\n",
    "            # print(\"te depadded_targets:\")\n",
    "            # print(depadded_targets)\n",
    "\n",
    "            # print(\"te depadded lengths:\")\n",
    "            # print(depadded_lengths)\n",
    "\n",
    "            # Train CTC accuracy in this batch\n",
    "            batch_acc = calculate_ctc_accuracy(outputs, target_lengths, depadded_targets, device)\n",
    "\n",
    "            # print(f\"te targets: {targets}\")\n",
    "            # print(f\"te targets.shape: {targets.shape}\")\n",
    "\n",
    "            # Loss function\n",
    "            loss = loss_fn(outputs, depadded_targets.float(), input_lengths, target_lengths)\n",
    "            # print(f\"te loss: {loss}, batch: {batch_idx}\")\n",
    "\n",
    "\n",
    "            # ###########################################################################################################\n",
    "\n",
    "            # # print(f\"te batch_idx: {batch_idx}\")\n",
    "\n",
    "            # # Send data to target device\n",
    "            # batch_mfccs = batch_mfccs.squeeze(1).permute(0, 2, 1).to(device)\n",
    "            # batch_targets = [target.to(device) for target in batch_targets]\n",
    "\n",
    "\n",
    "            # #if len(batch_mfccs) < batch_idx:\n",
    "            # #    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # # Forward pass\n",
    "\n",
    "            # # print(f\"te batch_mfccs: {batch_mfccs}\")\n",
    "            # # print(f\"te batch_targets: {batch_targets}\")\n",
    "\n",
    "            # outputs = model(batch_mfccs)\n",
    "            # # print(f\"te outputs from model(batch_mfccs): {outputs}\")\n",
    "            # # print(f\"te outputs.shape straight outta compton: {outputs.shape}\")\n",
    "\n",
    "            # outputs = torch.permute(outputs, (1, 0, 2))\n",
    "            # # print(f\"te batch_mfccs.size(2): {batch_mfccs.size(2)}\")\n",
    "            # # print(f\"te outputs.premuted.shape: {outputs.shape} || CTCLoss expects: [T max time length, N batch_size, C num_classes]\")\n",
    "\n",
    "\n",
    "            # outputs = outputs.view(batch_mfccs.size(2), len(batch_mfccs), num_classes)\n",
    "            # # print(f\"te outputs shape view: {outputs.shape}\")\n",
    "\n",
    "\n",
    "            # #outputs = F.log_softmax(outputs, dim=2)\n",
    "\n",
    "            # # input_lengths = torch.tensor([mfcc.shape[1] for mfcc in batch_mfccs])\n",
    "            # # input_lengths_list = input_lengths.tolist()\n",
    "\n",
    "            # #input_lengths = torch.tensor([mfcc.shape[1] for mfcc in batch_mfccs])\n",
    "\n",
    "            # #target_lengths = torch.tensor([len(trans) for trans in batch_targets], dtype=torch.long)\n",
    "\n",
    "\n",
    "            # #input_lengths = torch.tensor([mfcc.shape[1] for mfcc in batch_mfccs])\n",
    "            # input_lengths = torch.tensor([outputs.shape[0] for _ in batch_mfccs])\n",
    "\n",
    "            # input_lengths_list = input_lengths.tolist()\n",
    "\n",
    "            # # Calculate target lengths excluding [PAD] tokens (index 1)\n",
    "            # target_lengths = torch.tensor([len(target) - (target == 1).sum().item() for target in batch_targets], dtype=torch.long)\n",
    "\n",
    "            # #target_lengths = torch.tensor([len(trans) for trans in batch_targets], dtype=torch.long)\n",
    "            # target_lengths_list = target_lengths.tolist()\n",
    "\n",
    "\n",
    "            # # Calculate the maximum sequence length\n",
    "            # max_sequence_length = max(input_lengths)\n",
    "\n",
    "\n",
    "            # # # Initialize an empty tensor to hold the padded targets\n",
    "            # # targets = torch.zeros(len(batch_targets), max_sequence_length, dtype=torch.long)\n",
    "\n",
    "            # # for i, target in enumerate(batch_targets):\n",
    "            # #     targets[i, :target.size(0)] = target  # Copy values into the targets tensor\n",
    "\n",
    "\n",
    "\n",
    "            # targets = torch.zeros(len(batch_targets), max_sequence_length, dtype=torch.long)\n",
    "\n",
    "            # for i, target in enumerate(batch_targets):\n",
    "            #     # Filter out [PAD] token (index 1)\n",
    "            #     target_filtered = target[target != 1]\n",
    "            #     targets[i, :target_filtered.size(0)] = target_filtered  # Copy values into the targets tensor\n",
    "\n",
    "            # # Fill in the padded target sequences\n",
    "            # for i, target_seq in enumerate(targets):  # Use the filtered targets instead\n",
    "            #     target_seq = target_seq[:target_lengths[i]]\n",
    "            #     targets[i, :target_lengths[i]] = target_seq\n",
    "\n",
    "\n",
    "\n",
    "            # # Test CTC accuracy in this batch\n",
    "            # batch_acc = calculate_ctc_accuracy(outputs, target_lengths, targets, device)\n",
    "\n",
    "            # # epsilon = 1e-8\n",
    "            # # outputs = torch.log(outputs + epsilon)\n",
    "            # #outputs = torch.log(outputs) ## apsolutno vitalno da ctcloss ne bude negativan bruh\n",
    "            # # print(f\"te log outputs: {outputs}\")\n",
    "\n",
    "\n",
    "            # # Fill in the padded target sequences\n",
    "            # # for i, target_seq in enumerate(batch_targets):\n",
    "            # #     targets[i, :target_lengths[i]] = target_seq\n",
    "\n",
    "\n",
    "            # # Loss function\n",
    "            # loss = loss_fn(outputs, targets.float(), input_lengths, target_lengths)\n",
    "            # print(f\"te loss: {loss}, batch: {batch_idx}\")\n",
    "            ###################################################################################\n",
    "\n",
    "\n",
    "            # Test total Loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Test total Accuracy\n",
    "            test_acc += batch_acc\n",
    "\n",
    "            # Print out status:\n",
    "            #tqdm.write(f\" Batch [{batch_idx+1}/{len(test_dataloader)}], test Loss: {loss.item():.4f}, test Accuracy: {batch_acc:.4f}\")\n",
    "            #sys.stdout.flush()\n",
    "\n",
    "            #test_iterator.set_description(f'Testing - Loss: {loss.item():.4f}, Acc: {batch_acc:.4f}')\n",
    "\n",
    "            predicted_labels = torch.argmax(outputs, dim=2)\n",
    "            #print(f\"te predicted_labels: {predicted_labels}, batch: {batch_idx}\")\n",
    "\n",
    "            predicted_seq = predicted_labels[:target_lengths[i], i].to(device)  # Trim padding from predicted sequence\n",
    "            #print(f\"te predicted_seq: {predicted_seq}, batch: {batch_idx}\")\n",
    "\n",
    "\n",
    "            reference_seq = targets[i][:target_lengths[i]].to(device) # Trim padding from reference sequence\n",
    "            #print(f\"te reference_seq: {reference_seq}, batch: {batch_idx}\")\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                print(\"Working on the first batch...\")\n",
    "\n",
    "                # print one example:\n",
    "                transcription = [idx_to_char[idx.item()] for idx in predicted_seq]\n",
    "                transcription = ''.join(transcription)\n",
    "                c_transcription = [idx_to_char[idx.item()] for idx in reference_seq]\n",
    "                c_transcription = ''.join(c_transcription)\n",
    "\n",
    "                # Print the transcription\n",
    "                print(f\"testfirst correct Transcription: \\n{c_transcription}\")\n",
    "                print(f\"testfirst Transcription: \\n{transcription}\")\n",
    "\n",
    "\n",
    "            elif batch_idx == total_test_batches // 2:\n",
    "                print(\"Working on the middle batch...\")\n",
    "\n",
    "                # print one example:\n",
    "                transcription = [idx_to_char[idx.item()] for idx in predicted_seq]\n",
    "                transcription = ''.join(transcription)\n",
    "                c_transcription = [idx_to_char[idx.item()] for idx in reference_seq]\n",
    "                c_transcription = ''.join(c_transcription)\n",
    "\n",
    "                # Print the transcription\n",
    "                print(f\"testmid correct Transcription: \\n{c_transcription}\")\n",
    "                print(f\"testmid Transcription: \\n{transcription}\")\n",
    "\n",
    "\n",
    "            elif batch_idx == total_test_batches - 1:\n",
    "                print(\"Working on the last batch...\")\n",
    "\n",
    "                # print one example:\n",
    "                transcription = [idx_to_char[idx.item()] for idx in predicted_seq]\n",
    "                transcription = ''.join(transcription)\n",
    "                c_transcription = [idx_to_char[idx.item()] for idx in reference_seq]\n",
    "                c_transcription = ''.join(c_transcription)\n",
    "\n",
    "                # Print the transcription\n",
    "                print(f\"testlast correct Transcription: \\n{c_transcription}\")\n",
    "                print(f\"testlast Transcription: \\n{transcription}\")\n",
    "\n",
    "\n",
    "\n",
    "            # counter = 0\n",
    "\n",
    "            # # PER BATCH? so every 20th batch is now:\n",
    "            # for i in range(predicted_labels.shape[1]):\n",
    "\n",
    "            #     counter += 1\n",
    "            #     if counter % 20 == 0:\n",
    "\n",
    "            #         #transcription = [idx_to_char[idx.item()] for idx in predicted_seq[:, i]]\n",
    "            #         transcription = [idx_to_char[idx.item()] for idx in predicted_seq]\n",
    "            #         transcription = ''.join(transcription)\n",
    "\n",
    "            #         #c_transcription = [idx_to_char[idx.item()] for idx in reference_seq[:, i]]\n",
    "            #         c_transcription = [idx_to_char[idx.item()] for idx in reference_seq]\n",
    "            #         c_transcription = ''.join(c_transcription)\n",
    "\n",
    "            #         # Print the transcription\n",
    "            #         print(f\"test100 correct Transcription: \\n{c_transcription}\")\n",
    "            #         print(f\"test100 Transcription: \\n{transcription}\")\n",
    "\n",
    "\n",
    "        # Average loss and acc per batch\n",
    "        test_loss = test_loss / len(test_dataloader)\n",
    "        test_acc = test_acc / len(test_dataloader)\n",
    "        # print(f\"counter {counter}\")\n",
    "        return test_loss, test_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## COMBINATION OF TRAIN AND TEST:\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          num_classes: int,\n",
    "          device: torch.device,) -> Dict[str, List]:\n",
    "\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for\n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]}\n",
    "    For example if training for epochs=2:\n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]}\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary with extra info\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": [],\n",
    "               \"duration_s\": [],\n",
    "               \"model\": str(model),\n",
    "               \"train_dataloader\": str(train_dataloader),\n",
    "               \"test_dataloader\": str(test_dataloader),\n",
    "               \"optimizer\": str(optimizer),\n",
    "               \"loss_fn\": str(loss_fn),\n",
    "               \"epochs\": str(epochs),\n",
    "               \"device\": str(device)\n",
    "    }\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "    #print(f\"started timer[{timer()}s]\")\n",
    "\n",
    "    # Make sure model on target device\n",
    "    model.to(device)\n",
    "    #print(f\"model on device[{timer()}s]\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### EPOCH LOOPING IS HERE ###\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        #print(f\"epochs: {epochs}\")\n",
    "\n",
    "        end_time = timer() - start_time\n",
    "        print(f\"[{end_time:.2f}s] about to do train_step, epoch {epoch+1}\")\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          train_dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          num_classes=num_classes,\n",
    "                                          device=device)\n",
    "\n",
    "        end_time = timer() - end_time\n",
    "        print(f\"[{end_time:.2f}s] about to do test_step, epoch {epoch+1}\")\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        test_dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        num_classes=num_classes,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "                      f\" Epoch: {epoch+1:02d} | \"\n",
    "                      f\"train_loss: {train_loss:.3f} | \"\n",
    "                      f\"train_acc: {train_acc:.3f} | \"\n",
    "                      f\"test_loss: {test_loss:.3f} | \"\n",
    "                      f\"test_acc: {test_acc:.3f}\",\n",
    "        )\n",
    "\n",
    "        #sys.stdout.flush()\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        # Empty GPU cache if batch size or len(train_dataloader) is larger than a threshold\n",
    "        #if device == \"cuda\" and (len(train_dataloader)*HIDDEN_UNITS > flush_cuda):\n",
    "        #    torch.cuda.empty_cache()\n",
    "\n",
    "    end_time = timer()\n",
    "    results[\"duration_s\"].append(end_time - start_time)\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1ba80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction.py\n",
    "\n",
    "## PREDICTION\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import model_builder\n",
    "from char_to_idx import idx_to_char, char_to_idx\n",
    "from typing import Tuple\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "from data_setup import MFCCWithoutZeroth, ASRDataset\n",
    "\n",
    "# # Creating a parser\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/models/\"\n",
    "model_pth_name = \"LiteGRUv2_model_test_12.pth\"\n",
    "\n",
    "\n",
    "# # Pick an audio path\n",
    "# parser.add_argument(\"--audio\",\n",
    "#                     default= \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/test\",\n",
    "#                     type=str,\n",
    "#                     help=\"target audio to transcribe!\")\n",
    "# # Pick a model path\n",
    "# parser.add_argument(\"--model_path\",\n",
    "#                     default= model_path + model_pth_name,\n",
    "#                     type=str,\n",
    "#                     help=\"provide folder and name of saved model to be used for prediction/transcription\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# # Get audio and model paths\n",
    "# AUDIO_PATH = args.audio\n",
    "# print(f\"[INFO] Transcribing: {AUDIO_PATH}\")\n",
    "# MOD_PATH = args.model_path\n",
    "# print(f\"[INFO] with model: {model_pth_name}\")\n",
    "\n",
    "\n",
    "\n",
    "AUDIO_PATH = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/test\"\n",
    "print(f\"[INFO] Transcribing: {AUDIO_PATH}\")\n",
    "MOD_PATH = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/models/\"\n",
    "print(f\"[INFO] with model: {model_pth_name}\")\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "### LOAD MODEL ###\n",
    "\n",
    "def load_model(filepath=MOD_PATH):\n",
    "  model=model_builder.LiteGRUv2(num_classes = 48,\n",
    "                                in_channels = n_mfccs-1,\n",
    "                                hidden_units = 256,\n",
    "                                dropout = 0.1).to(device)\n",
    "\n",
    "  print(f\"[INFO] Loading model...\")\n",
    "\n",
    "  # Load the model's state dictionary and map tensors to CPU\n",
    "  model.load_state_dict(torch.load(filepath, map_location=torch.device('cpu')))\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### TRANSCRIBE TEXT ###\n",
    "\n",
    "def transcribe(model: torch.nn.Module,\n",
    "               audio_data: int,\n",
    "               device: torch.device) -> str:\n",
    "\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        print(f\"pred audio_data.shape raw: {audio_data.shape}\")\n",
    "\n",
    "        audio_data = audio_data.squeeze(1).permute(0, 2, 1).to(device)\n",
    "        print(f\"pred audio_data.shape after: {audio_data.shape}\")\n",
    "\n",
    "        outputs = model(audio_data)\n",
    "\n",
    "        print(f\"pred outputs.shape: {outputs.shape}\")\n",
    "        print(f\"pred output logits: {outputs}\")\n",
    "\n",
    "        outputs = F.log_softmax(outputs, dim=2)\n",
    "        print(f\"pred log_softmax outputs: {outputs}\")\n",
    "\n",
    "        predicted_labels = torch.argmax(outputs, dim=2)\n",
    "        print(f\"pred predicted_labels.shape: {predicted_labels.shape}\")\n",
    "        print(f\"pred predicted_labels after argmax: {predicted_labels}\")\n",
    "\n",
    "        #transcription = [idx_to_char[idx] for idx in predicted_labels]\n",
    "        transcription = [idx_to_char[idx.item()] for idx in predicted_labels[0]]  # Take the first sequence\n",
    "\n",
    "        transcription = ''.join(transcription)\n",
    "\n",
    "        # Print the transcription\n",
    "        print(\"pred Transcription:\", transcription)\n",
    "\n",
    "    return transcription\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_mfccs = 13\n",
    "\n",
    "model = load_model(MOD_PATH+model_pth_name)\n",
    "\n",
    "\n",
    "mfcc_transform = MFCCWithoutZeroth(sample_rate = 16000,\n",
    "                                   n_mfcc = n_mfccs,\n",
    "                                   melkwargs = {\"n_fft\": 400,\n",
    "                                              \"hop_length\": 160,\n",
    "                                              \"n_mels\": 40}\n",
    ")\n",
    "\n",
    "\n",
    "test_dir = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/test\"\n",
    "test_tran = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/test_trans_validated.txt\"\n",
    "\n",
    "\n",
    "# Create an instance of your dataset\n",
    "trans_data = ASRDataset(test_dir,\n",
    "                        test_tran,\n",
    "                        mfcc_transform,\n",
    "                        char_to_idx)\n",
    "\n",
    "num_samples = len(trans_data)\n",
    "\n",
    "# Generate a random index within the range of available samples\n",
    "#random_idx = random.randint(0, num_samples - 1)\n",
    "random_idx = 619\n",
    "\n",
    "# Fetch the data sample using the random index\n",
    "sample_mfcc, sample_label = trans_data[random_idx]\n",
    "\n",
    "\n",
    "transcrip = transcribe(model, sample_mfcc, device)\n",
    "\n",
    "print(f\"TRANSCRIPTION: {transcrip}\")\n",
    "print(f\"CORRECT: {sample_label}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   transcribe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa54ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "\"\"\"\n",
    "Trains a PyTorch ASR LiteGRUv2 model. New build or do additional training.\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import data_setup, engine, model_builder, utils\n",
    "import torchaudio.transforms as T\n",
    "from data_setup import MFCCWithoutZeroth, create_dataloaders\n",
    "import char_to_idx\n",
    "\n",
    "\n",
    "NUM_CLASSES = len(char_to_idx.char_to_idx)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Get hyperparameters\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--num_epochs\", default=3, type=int, help=\"the number of epochs to train for\")\n",
    "parser.add_argument(\"--batch_size\", default=64, type=int, help=\"number of samples per batch\")\n",
    "parser.add_argument(\"--hidden_units\", default=512, type=int, help=\"number of hidden units in hidden layers\")\n",
    "parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"the learning rate for the model\")\n",
    "\n",
    "# Add the option for \"new\" or \"old\" model\n",
    "parser.add_argument(\"--model_option\", choices=[\"new\", \"old\"], default=\"old\", help=\"Train a new or old model\")\n",
    "parser.add_argument(\"--old_model_path\", type=str, help=\"path to the old model checkpoint\")\n",
    "\n",
    "parser.add_argument(\"--n_mfccs\", default=13, type=int, help=\"number of MFCC coefficients\")\n",
    "parser.add_argument(\"--dropout\", default=0.1, type=float, help=\"dropout rate\")\n",
    "parser.add_argument(\"--train_dir\", default=r\"C:\\Users\\Mr Cab Driver\\Documents\\aCreativeHub\\Projekti\\Speech-to-Text\\LiteGRUv1\\cv-corpus-14.0-delta-2023-06-23-en\\cv-corpus-14.0-delta-2023-06-23\\en\\clips_4k_set_only_validated\\train\", type=str, help=\"the train data directory\")\n",
    "parser.add_argument(\"--test_dir\", default=r\"C:\\Users\\Mr Cab Driver\\Documents\\aCreativeHub\\Projekti\\Speech-to-Text\\LiteGRUv1\\cv-corpus-14.0-delta-2023-06-23-en\\cv-corpus-14.0-delta-2023-06-23\\en\\clips_4k_set_only_validated\\test\", type=str, help=\"the test data directory\")\n",
    "parser.add_argument(\"--train_tran\", default=r\"C:\\Users\\Mr Cab Driver\\Documents\\aCreativeHub\\Projekti\\Speech-to-Text\\LiteGRUv1\\cv-corpus-14.0-delta-2023-06-23-en\\cv-corpus-14.0-delta-2023-06-23\\en\\clips_4k_set_only_validated\\train_trans_validated.txt\", type=str, help=\"the train data directory\")\n",
    "parser.add_argument(\"--test_tran\", default=r\"C:\\Users\\Mr Cab Driver\\Documents\\aCreativeHub\\Projekti\\Speech-to-Text\\LiteGRUv1\\cv-corpus-14.0-delta-2023-06-23-en\\cv-corpus-14.0-delta-2023-06-23\\en\\clips_4k_set_only_validated\\test_trans_validated.txt\", type=str, help=\"the test data directory\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.model_option == \"new\":\n",
    "  print(f\"[INFO] Training a NEW model from scratch!\")\n",
    "else:\n",
    "  print(f\"[INFO] Training an OLD model from old_model_path!\")\n",
    "\n",
    "\n",
    "# Setup hyperparameters\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "HIDDEN_UNITS = args.hidden_units\n",
    "LEARNING_RATE = args.learning_rate\n",
    "N_MFCCS = args.n_mfccs\n",
    "DROPOUT = args.dropout\n",
    "TRAIN_DIR = args.train_dir\n",
    "TEST_DIR = args.test_dir\n",
    "TRAIN_TRAN = args.train_tran\n",
    "TEST_TRAN = args.test_tran\n",
    "OLD_MODEL_PATH = args.old_model_path\n",
    "\n",
    "\n",
    "\n",
    "# flush_cuda = 65537 # 64 batch * 1024 hidden +1\n",
    "\n",
    "print(f\"[INFO] Training a model for {NUM_EPOCHS} epochs with a batch size {BATCH_SIZE} using {HIDDEN_UNITS} hidden units and a learning rate of {LEARNING_RATE}\\n\")\n",
    "\n",
    "# Setup directories\n",
    "train_dir = args.train_dir\n",
    "test_dir = args.test_dir\n",
    "train_tran = args.train_tran\n",
    "test_tran = args.test_tran\n",
    "\n",
    "n_train = sum(1 for filename in os.listdir(train_dir) if filename.lower().endswith(('.mp3', '.wav')))\n",
    "n_test = sum(1 for filename in os.listdir(test_dir) if filename.lower().endswith(('.mp3', '.wav')))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"[INFO] Training data: {n_train} audio files \\n{train_dir}\")\n",
    "print(f\"[INFO] Testing data: {n_test} audio files \\n{test_dir}\")\n",
    "\n",
    "print(f\"[INFO] Training data transcriptions: \\n{train_tran}\")\n",
    "print(f\"[INFO] Testing data transcriptions: \\n{test_tran}\\n\")\n",
    "\n",
    "print(f\"[INFO] char_to_idx contains: {NUM_CLASSES} classes\\n\")\n",
    "\n",
    "# Setup target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] device set to: {device}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Create the MFCC transformer object\n",
    "mfcc_transform = MFCCWithoutZeroth(sample_rate = 16000,\n",
    "                                   n_mfcc = N_MFCCS,\n",
    "                                   melkwargs = {\"n_fft\": 400,\n",
    "                                              \"hop_length\": 160,\n",
    "                                              \"n_mels\": 40}\n",
    ")\n",
    "\n",
    "print(f\"[INFO] mfcc_transform active: {mfcc_transform}\\n\")\n",
    "print(f\"[INFO] n_mfccs output: {N_MFCCS-1}\\n\")\n",
    "\n",
    "# Create DataLoaders with help from data_setup.py\n",
    "train_dataloader, test_dataloader = data_setup.create_dataloaders(\n",
    "                                      train_dir=train_dir,\n",
    "                                      train_tran=train_tran,\n",
    "                                      test_dir=test_dir,\n",
    "                                      test_tran=test_tran,\n",
    "                                      mfcc_transform=mfcc_transform,\n",
    "                                      batch_size=BATCH_SIZE\n",
    "  )\n",
    "\n",
    "print(f\"[INFO] train_dataloader active: {train_dataloader}\\n\")\n",
    "print(f\"[INFO] test_dataloader active: {test_dataloader}\\n\")\n",
    "\n",
    "print(f\"[INFO] preparing model: {test_dataloader}\\n\")\n",
    "\n",
    "\n",
    "def load_model_with_params(model_pth_path):\n",
    "    # Derive the .pkl file path from the .pth file path\n",
    "    model_pkl_path = model_pth_path[:-3] + 'pkl'\n",
    "\n",
    "    # Load the state dictionary from the .pth file\n",
    "    model_state_dict = torch.load(model_pth_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # Load additional model information from the .pkl file\n",
    "    with open(model_pkl_path, 'rb') as f:\n",
    "        model_info = pickle.load(f)\n",
    "\n",
    "    # Extract model architecture details\n",
    "    model_architecture = model_info['model']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Use regular expressions to find and extract the necessary values\n",
    "    in_channels_match = re.search(r'Conv1d\\((\\d+)', model_architecture)\n",
    "    if in_channels_match:\n",
    "        in_channels = int(in_channels_match.group(1))\n",
    "    else:\n",
    "        raise ValueError(\"Could not find in_channels in the model architecture.\")\n",
    "\n",
    "    hidden_units_match = re.search(r'GRU\\((\\d+)', model_architecture)\n",
    "    if hidden_units_match:\n",
    "        hidden_units = int(hidden_units_match.group(1))\n",
    "    else:\n",
    "        raise ValueError(\"Could not find hidden_units in the model architecture.\")\n",
    "\n",
    "    dropout_match = re.search(r'Dropout\\(p=([\\d.]+)', model_architecture)\n",
    "    if dropout_match:\n",
    "        dropout = float(dropout_match.group(1))\n",
    "    else:\n",
    "        raise ValueError(\"Could not find dropout in the model architecture.\")\n",
    "\n",
    "    # Other architecture details (e.g., out_features)\n",
    "    out_features_match = re.search(r'out_features=(\\d+)', model_architecture)\n",
    "    if out_features_match:\n",
    "        out_features = int(out_features_match.group(1))\n",
    "    else:\n",
    "        raise ValueError(\"Could not find out_features in the model architecture.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #num_classes = model_info['num_classes']\n",
    "    #in_channels = model_info['in_channels']\n",
    "    #hidden_units = model_info['hidden_units']\n",
    "    #dropout = model_info['dropout']\n",
    "\n",
    "    model = model_builder.LiteGRUv2(num_classes=NUM_CLASSES,\n",
    "                                    in_channels=in_channels,\n",
    "                                    hidden_units=hidden_units,\n",
    "                                    dropout=dropout).to(torch.device('cpu'))\n",
    "\n",
    "    # Extract other relevant information from model_info if needed\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if args.model_option == \"new\":\n",
    "    model = model_builder.LiteGRUv2(num_classes=NUM_CLASSES,\n",
    "                                    in_channels=N_MFCCS-1,\n",
    "                                    hidden_units=HIDDEN_UNITS,\n",
    "                                    dropout=DROPOUT).to(torch.device('cpu'))\n",
    "\n",
    "    print(f\"[INFO] NEW model active: {type(model).__name__}\\n\")\n",
    "\n",
    "else:\n",
    "    # Train an old model\n",
    "    if args.model_option == \"old\":\n",
    "        if args.old_model_path is None:\n",
    "            parser.error(\"--old_model_path argument is required when using the old model option\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Create an instance of the old model\n",
    "        model = load_model_with_params(\"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/models/\"+args.old_model_path)\n",
    "\n",
    "        print(f\"[INFO] OLD model active: {type(model).__name__}\\n\")\n",
    "\n",
    "\n",
    "# Apply weight initialization (e.g., Xavier/Glorot initialization)\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "# Set loss and optimizer\n",
    "loss_fn = torch.nn.CTCLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"[INFO] loss_fn active: {loss_fn}\")\n",
    "\n",
    "print(f\"[INFO] optimizer active: {optimizer}\\n\")\n",
    "\n",
    "print(f\"[INFO] about to train...\\n\")\n",
    "\n",
    "# Start training with help from engine.py\n",
    "log_book = engine.train(model=model,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optimizer,\n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        num_classes=NUM_CLASSES,\n",
    "                        device=device,\n",
    "                        )\n",
    "\n",
    "print(\"[INFO] did the train\\n\")\n",
    "\n",
    "print(f\"log_book: {log_book}\\n\")\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "\n",
    "# The base filename you want to use\n",
    "base_filename = \"LiteGRUv2_model_test.pth\"\n",
    "\n",
    "# The directory where you want to save the models\n",
    "target_dir = \"/content/drive/MyDrive/pytorch/LiteGRUv1/cv-corpus-14.0-delta-2023-06-23/en/models\"\n",
    "\n",
    "\n",
    "# Check if the base filename exists\n",
    "if os.path.exists(os.path.join(target_dir, base_filename)):\n",
    "    # If the base filename exists, find the next available number\n",
    "    # Loop until you find a unique filename\n",
    "    file_number = 1\n",
    "    while True:\n",
    "        new_filename = f\"{base_filename[:-4]}_{file_number}.pth\"\n",
    "        if not os.path.exists(os.path.join(target_dir, new_filename)):\n",
    "            break\n",
    "        file_number += 1\n",
    "else:\n",
    "    # If the base filename doesn't exist, use it directly\n",
    "    new_filename = base_filename\n",
    "\n",
    "# Save the model with the new filename\n",
    "utils.save_model(model=model,\n",
    "                 target_dir=target_dir,\n",
    "                 model_name=new_filename\n",
    "                 )\n",
    "\n",
    "dict_save_path = os.path.join(target_dir, f\"{new_filename[:-4]}.pkl\")  # Change extension to .pkl\n",
    "\n",
    "# Save the Log book using pickle\n",
    "with open(dict_save_path, 'wb') as f:\n",
    "    pickle.dump(log_book, f)\n",
    "\n",
    "print(f\"[INFO] log_book has been pickled as {new_filename[:-4]}.pkl \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975424b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71d88d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3a837d3dab48acb680ddb125ce8d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='5', description='Number of epochs:', style=TextStyle(description_width='initial')),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4991d6bf610940cea26b310517bf5650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%writefile main_prompt.py\n",
    "import multiprocessing\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    OLD_MODEL = \"LiteGRUv2_model_test_12.pth\"\n",
    "\n",
    "    # Default values\n",
    "    def_epochs = 5\n",
    "    def_batch = 64\n",
    "    def_hidden = 512\n",
    "    def_learn = 0.001\n",
    "    options = [\"old\", \"new\"]\n",
    "    def_model = \"new\"\n",
    "    def_n_mfccs = 13\n",
    "    def_dropout = 0.1\n",
    "\n",
    "    # Create the input widgets\n",
    "    input_variable_1 = widgets.Text(description='Number of epochs:', value=str(def_epochs), style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_2 = widgets.Text(description='Batch size:', value=str(def_batch), style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_3 = widgets.Text(description='Hidden units in layer:', value=str(def_hidden), style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_4 = widgets.Text(description='Learning rate:', value=str(def_learn), style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_5 = widgets.RadioButtons(description='Train \"old\" or \"new\" model?', options=options, value=def_model, style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_6 = widgets.Text(description='Name of old model:', value=OLD_MODEL, style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_7 = widgets.IntText(description='Number of MFCC coefficients:', value=def_n_mfccs, style={'description_width': 'initial'}, align_self='center')\n",
    "    input_variable_8 = widgets.FloatText(description='Dropout rate:', value=def_dropout, style={'description_width': 'initial'}, align_self='center')\n",
    "\n",
    "    # Create a button widget\n",
    "    run_button = widgets.Button(description='Run Script')\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            print(f\"Button clicked, script running...\")\n",
    "\n",
    "            clear_output()  # Clear the output area before running the script\n",
    "            variable_1 = input_variable_1.value or def_epochs\n",
    "            variable_2 = input_variable_2.value or def_batch\n",
    "            variable_3 = input_variable_3.value or def_hidden\n",
    "            variable_4 = input_variable_4.value or def_learn\n",
    "            variable_5 = input_variable_5.value or def_model\n",
    "            variable_6 = input_variable_6.value or OLD_MODEL\n",
    "            variable_7 = input_variable_7.value or def_n_mfccs\n",
    "            variable_8 = input_variable_8.value or def_dropout\n",
    "\n",
    "            # Construct the command to run the .py script using the entered variables IN ORDER ARE:\n",
    "            command = f'python train.py --num_epochs {variable_1} --batch_size {variable_2} --hidden_units {variable_3} --learning_rate {variable_4}'\n",
    "\n",
    "            command += f' --model_option {variable_5}'\n",
    "\n",
    "            command += f' --old_model_path {variable_6}'\n",
    "\n",
    "            command += f' --n_mfccs {variable_7} --dropout {variable_8}'\n",
    "\n",
    "            # Run the command in a subprocess with real-time output\n",
    "            process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "            for line in process.stdout:\n",
    "                print(line, end='')\n",
    "\n",
    "    run_button.on_click(on_button_clicked)\n",
    "\n",
    "    # Create a VBox layout for the input fields and button\n",
    "    input_layout = widgets.VBox([input_variable_1, input_variable_2, input_variable_3, input_variable_4, input_variable_5, input_variable_6, run_button])\n",
    "\n",
    "    # Display the form\n",
    "    display(input_layout, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c3c1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6586ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea3d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a18d1522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 4C8C-72CB\n",
      "\n",
      " Directory of C:\\Users\\Mr Cab Driver\\Documents\\aCreativeHub\\Projekti\\Speech-to-Text\\LiteGRUv1\\anaconda\\LiteGRUv2_v2_7\n",
      "\n",
      "16.08.2023.  00:37    <DIR>          .\n",
      "16.08.2023.  00:37    <DIR>          ..\n",
      "16.08.2023.  00:35               589 char_to_idx.py\n",
      "15.08.2023.  22:43             2.084 combine_and_plot.py\n",
      "16.08.2023.  00:35             8.688 data_setup.py\n",
      "16.08.2023.  00:35            25.881 engine.py\n",
      "16.08.2023.  00:37            82.789 LiteGRUv2_v2.7_transfer_from_colab.ipynb\n",
      "15.08.2023.  23:31            14.014 LiteGRUv2_v2_7.7z\n",
      "15.08.2023.  23:20             3.460 main_prompt.py\n",
      "16.08.2023.  00:35             2.242 model_builder.py\n",
      "16.08.2023.  00:35             3.447 plot_loss_curves.py\n",
      "16.08.2023.  00:35             4.747 prediction.py\n",
      "16.08.2023.  00:35            10.575 train.py\n",
      "16.08.2023.  00:35             3.086 utils.py\n",
      "15.08.2023.  23:49    <DIR>          __pycache__\n",
      "              12 File(s)        161.602 bytes\n",
      "               3 Dir(s)  32.043.143.168 bytes free\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6fed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
