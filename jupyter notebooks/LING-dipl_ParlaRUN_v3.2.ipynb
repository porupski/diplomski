{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i41M-W8THOQu"
   },
   "source": [
    "# Running ParlaSpeechHR Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESbAn_EQRYyT"
   },
   "source": [
    "## [0] Start here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:30:29.125370Z",
     "iopub.status.busy": "2024-06-15T19:30:29.125370Z",
     "iopub.status.idle": "2024-06-15T19:30:29.131859Z",
     "shell.execute_reply": "2024-06-15T19:30:29.131859Z",
     "shell.execute_reply.started": "2024-06-15T19:30:29.125370Z"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1718358967568,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "hXbSzO_hGn2a",
    "outputId": "93214040-342d-47cd-b2d5-ef7bf1304c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3.2\n"
     ]
    }
   ],
   "source": [
    "work_version_num = 3.2\n",
    "word_version_preffix = \"\"\n",
    "word_version_suffix = \"\"\n",
    "work_version = f\"{word_version_preffix}v{work_version_num}{word_version_suffix}\"\n",
    "\n",
    "print(work_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:30:30.905317Z",
     "iopub.status.busy": "2024-06-15T19:30:30.905317Z",
     "iopub.status.idle": "2024-06-15T19:30:30.911655Z",
     "shell.execute_reply": "2024-06-15T19:30:30.911655Z",
     "shell.execute_reply.started": "2024-06-15T19:30:30.905317Z"
    }
   },
   "outputs": [],
   "source": [
    "choose_lm_model = False\n",
    "\n",
    "google_colabbing = False\n",
    "debug_printing = False\n",
    "\n",
    "testing_asr = True # if False, cuts audio into chunks\n",
    "shuffle_transcript = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [INFO ABOUT MACHINE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T17:50:12.377014Z",
     "iopub.status.busy": "2024-06-15T17:50:12.377014Z",
     "iopub.status.idle": "2024-06-15T17:50:12.379877Z",
     "shell.execute_reply": "2024-06-15T17:50:12.379877Z",
     "shell.execute_reply.started": "2024-06-15T17:50:12.377014Z"
    }
   },
   "outputs": [],
   "source": [
    "### NEED TO REINSTALL PYTORCH WITH CUDA ENABLED AND TRY AGAIN I GUESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T17:50:12.778026Z",
     "iopub.status.busy": "2024-06-15T17:50:12.777029Z",
     "iopub.status.idle": "2024-06-15T17:50:13.745492Z",
     "shell.execute_reply": "2024-06-15T17:50:13.745492Z",
     "shell.execute_reply.started": "2024-06-15T17:50:12.778026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: NVIDIA CUDA\n",
      "Vendor: NVIDIA Corporation\n",
      "Version: OpenCL 1.2 CUDA 11.1.96\n",
      "\n",
      "Device: GeForce GTX 760\n",
      "Type: ALL | GPU\n",
      "Max Compute Units: 6\n",
      "Max Work Item Dimensions: 3\n",
      "Max Work Group Size: 1024\n",
      "Max Clock Frequency: 1150 MHz\n",
      "Global Memory Size: 4.00 GB\n",
      "Local Memory Size: 48.00 KB\n",
      "Max Memory Allocation Size: 1.00 GB\n",
      "OpenCL Version: OpenCL C 1.2 \n",
      "Driver Version: 456.71\n",
      "Platform: Intel(R) OpenCL HD Graphics\n",
      "Vendor: Intel(R) Corporation\n",
      "Version: OpenCL 3.0 \n",
      "\n",
      "Device: Intel(R) UHD Graphics 750\n",
      "Type: ALL | GPU\n",
      "Max Compute Units: 32\n",
      "Max Work Item Dimensions: 3\n",
      "Max Work Group Size: 256\n",
      "Max Clock Frequency: 1300 MHz\n",
      "Global Memory Size: 6.32 GB\n",
      "Local Memory Size: 64.00 KB\n",
      "Max Memory Allocation Size: 3.16 GB\n",
      "OpenCL Version: OpenCL C 1.2 \n",
      "Driver Version: 30.0.101.1273\n",
      "Platform: Intel(R) OpenCL\n",
      "Vendor: Intel(R) Corporation\n",
      "Version: OpenCL 3.0 WINDOWS\n",
      "\n",
      "Device: 11th Gen Intel(R) Core(TM) i5-11600 @ 2.80GHz\n",
      "Type: ALL | CPU\n",
      "Max Compute Units: 12\n",
      "Max Work Item Dimensions: 3\n",
      "Max Work Group Size: 8192\n",
      "Max Clock Frequency: 2800 MHz\n",
      "Global Memory Size: 15.80 GB\n",
      "Local Memory Size: 32.00 KB\n",
      "Max Memory Allocation Size: 7.90 GB\n",
      "OpenCL Version: OpenCL C 3.0 \n",
      "Driver Version: 2023.16.6.0.28_042959\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyopencl\n",
    "\n",
    "import pyopencl as cl\n",
    "\n",
    "# Get a list of all available platforms\n",
    "platforms = cl.get_platforms()\n",
    "\n",
    "for platform in platforms:\n",
    "    print(f\"Platform: {platform.name}\")\n",
    "    print(f\"Vendor: {platform.vendor}\")\n",
    "    print(f\"Version: {platform.version}\")\n",
    "\n",
    "    # Get a list of all available devices for the current platform\n",
    "    devices = platform.get_devices()\n",
    "    for device in devices:\n",
    "        print(f\"\\nDevice: {device.name}\")\n",
    "        print(f\"Type: {cl.device_type.to_string(device.type)}\")\n",
    "        print(f\"Max Compute Units: {device.max_compute_units}\")\n",
    "        print(f\"Max Work Item Dimensions: {device.max_work_item_dimensions}\")\n",
    "        print(f\"Max Work Group Size: {device.max_work_group_size}\")\n",
    "        print(f\"Max Clock Frequency: {device.max_clock_frequency} MHz\")\n",
    "        print(f\"Global Memory Size: {device.global_mem_size / (1024 ** 3):.2f} GB\")\n",
    "        print(f\"Local Memory Size: {device.local_mem_size / 1024:.2f} KB\")\n",
    "        print(f\"Max Memory Allocation Size: {device.max_mem_alloc_size / (1024 ** 3):.2f} GB\")\n",
    "        print(f\"OpenCL Version: {device.opencl_c_version}\")\n",
    "        print(f\"Driver Version: {device.driver_version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU VERSION:\n",
    "Device: 11th Gen Intel(R) Core(TM) i5-11600 @ 2.80GHz\r\n",
    "Type: ALL | CPU\r\n",
    "Max Compute Units: 12\r\n",
    "Max Work Item Dimensions: 3\r\n",
    "Max Work Group Size: 8192\r\n",
    "Max Clock Frequency: 2800 MHz\r\n",
    "Global Memory Size: 15.80 GB\r\n",
    "Local Memory Size: 32.00 KB\r\n",
    "Max Memory Allocation Size: 7.90 GB\r\n",
    "OpenCL Version: OpenCL C 3.0 \r\n",
    "Driver Version: 2023.16.6.0.28_042959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T22:41:43.549482Z",
     "iopub.status.busy": "2024-06-15T22:41:43.548484Z",
     "iopub.status.idle": "2024-06-15T22:41:50.732110Z",
     "shell.execute_reply": "2024-06-15T22:41:50.732110Z",
     "shell.execute_reply.started": "2024-06-15T22:41:43.549482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.3.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, mkl, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, optimum, sentence-transformers, torchaudio, torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\pandas-2.1.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchvision\n",
      "Version: 0.15.2a0\n",
      "Summary: image and video datasets and models for torch deep learning\n",
      "Home-page: https://github.com/pytorch/vision\n",
      "Author: PyTorch Core Team\n",
      "Author-email: soumith@pytorch.org\n",
      "License: BSD\n",
      "Location: C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\n",
      "Requires: numpy, pillow, requests, torch\n",
      "Required-by: sentence-transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\pandas-2.1.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torchaudio\n",
      "Version: 2.3.1\n",
      "Summary: An audio package for PyTorch\n",
      "Home-page: https://github.com/pytorch/audio\n",
      "Author: Soumith Chintala, David Pollack, Sean Naren, Peter Goldsborough, Moto Hira, Caroline Chen, Jeff Hwang, Zhaoheng Ni, Xiaohui Zhang\n",
      "Author-email: soumith@pytorch.org\n",
      "License: \n",
      "Location: C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\n",
      "Requires: torch\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\pandas-2.1.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip show torch\n",
    "!pip show torchvision\n",
    "!pip show torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T18:17:22.441732Z",
     "iopub.status.busy": "2024-06-15T18:17:22.440735Z",
     "iopub.status.idle": "2024-06-15T18:17:22.446005Z",
     "shell.execute_reply": "2024-06-15T18:17:22.446005Z",
     "shell.execute_reply.started": "2024-06-15T18:17:22.441732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available or GPU is too old.\n",
      "device set to: CPU (cpu)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 3.7:\n",
    "    device = torch.device(\"cuda:0\")  # Use GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU\n",
    "    print(\"CUDA not available or GPU is too old.\")\n",
    "    print(f\"device set to: CPU ({device})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0vOINi-RVG-"
   },
   "source": [
    "## [1] Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T17:50:25.077956Z",
     "iopub.status.busy": "2024-06-15T17:50:25.077956Z",
     "iopub.status.idle": "2024-06-15T17:50:27.044499Z",
     "shell.execute_reply": "2024-06-15T17:50:27.044499Z",
     "shell.execute_reply.started": "2024-06-15T17:50:25.077956Z"
    },
    "executionInfo": {
     "elapsed": 4872,
     "status": "ok",
     "timestamp": 1718358985791,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "Y-1HeiN2HWYh",
    "outputId": "3e4fc84f-371f-455b-cafb-e8d2fcd8837a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good. Every essential package is present to make the code work.\n"
     ]
    }
   ],
   "source": [
    "#!pip install ipython\n",
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers: All good.\")\n",
    "except ImportError:\n",
    "    !pip install transformers\n",
    "    print(\"transformers: Installed.\")\n",
    "\n",
    "try:\n",
    "    import pydub\n",
    "    print(\"pydub: All good.\")\n",
    "except ImportError:\n",
    "    !pip install pydub\n",
    "    print(\"pydub: Installed.\")\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    print(\"torchaudio: All good.\")\n",
    "except ImportError:\n",
    "    !pip install torchaudio\n",
    "    print(\"torchaudio: Installed.\")\n",
    "\n",
    "try:\n",
    "    import pyctcdecode\n",
    "    print(\"pyctcdecode: All good.\")\n",
    "except ImportError:\n",
    "    !pip install pyctcdecode\n",
    "    print(\"pyctcdecode: Installed.\")\n",
    "\n",
    "try:\n",
    "    import jiwer\n",
    "    print(\"jiwer: All good.\")\n",
    "except ImportError:\n",
    "    !pip install jiwer\n",
    "    print(\"jiwer: Installed.\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    print(\"nltk: All good.\")\n",
    "except ImportError:\n",
    "    !pip install nltk\n",
    "    print(\"nltk: Installed.\")\n",
    "\n",
    "try:\n",
    "    from fuzzywuzzy import fuzz\n",
    "    print(\"fuzzywuzzy: All good.\")\n",
    "except ImportError:   \n",
    "    !pip install fuzzywuzzy\n",
    "    print(\"fuzzywuzzy: Installed.\")\n",
    "\n",
    "clear_output(wait=True) # clean the mess\n",
    "print(f\"All good. Every essential package is present to make the code work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T17:50:31.880251Z",
     "iopub.status.busy": "2024-06-15T17:50:31.880251Z",
     "iopub.status.idle": "2024-06-15T17:50:31.886159Z",
     "shell.execute_reply": "2024-06-15T17:50:31.886159Z",
     "shell.execute_reply.started": "2024-06-15T17:50:31.880251Z"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1718358989147,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "QpXmeQkhHSMy",
    "outputId": "18d66a32-7b49-4cac-ed0e-2f4d4bb4c771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good. kenlm is present to make the LM model.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import kenlm\n",
    "    print(f\"All good. kenlm is present to make the LM model.\")\n",
    "except ImportError:\n",
    "    !pip install https://github.com/kpu/kenlm/archive/master.zip # requires runtime restart\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9) # restart automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bkH_9mFRi5n"
   },
   "source": [
    "## [2] Defining all paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:30:33.684762Z",
     "iopub.status.busy": "2024-06-15T19:30:33.684762Z",
     "iopub.status.idle": "2024-06-15T19:30:33.688088Z",
     "shell.execute_reply": "2024-06-15T19:30:33.688088Z",
     "shell.execute_reply.started": "2024-06-15T19:30:33.684762Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1718358968575,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "PnFUfYq5HPsy"
   },
   "outputs": [],
   "source": [
    "path_to_pshr_raw_audio_data = r\"D:\\ParlaSpeech-HR.v1.0\\raw\"\n",
    "path_to_pshr_raw_jsonl = r\"D:\\ParlaSpeech-HR.v1.0\\ParlaSpeech-HR.v1.0.jsonl\"\n",
    "\n",
    "path_to_pshr_models = r\"D:\\ASR\\ParlaspeechHR\"\n",
    "path_to_pshr_wav2vec2_l = r\"D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavih-hr\\wav2vec2-large-slavic-parlaspeech-hr\"\n",
    "path_to_pshr_wav2vec2_l_lm = r\"D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavic-hr-lm\"\n",
    "\n",
    "test_audio_path = r\"D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavic-hr-lm\\nela_film_review.ogg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:30:33.964396Z",
     "iopub.status.busy": "2024-06-15T19:30:33.964396Z",
     "iopub.status.idle": "2024-06-15T19:30:33.967171Z",
     "shell.execute_reply": "2024-06-15T19:30:33.967171Z",
     "shell.execute_reply.started": "2024-06-15T19:30:33.964396Z"
    },
    "executionInfo": {
     "elapsed": 2210,
     "status": "ok",
     "timestamp": 1718358996758,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "aDFTbiSfHyu9",
    "outputId": "828f1e69-1cba-46eb-a187-0ad01bc92414"
   },
   "outputs": [],
   "source": [
    "# # Mount gdrive and place path (private, no download)\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# model_file_path = \"/content/drive/MyDrive/pytorch/wav2vec2-large-slavic-hr\"\n",
    "# output_folder = \"/content/drive/MyDrive/pytorch/wav2vec2-large-slavic-hr/temp\"\n",
    "\n",
    "# model_file_path_lm = \"/content/drive/MyDrive/pytorch/wav2vec2-large-slavic-hr-lm\"\n",
    "# output_folder_lm = \"/content/drive/MyDrive/pytorch/wav2vec2-large-slavic-hr-lm/temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKlYE54RRsUH"
   },
   "source": [
    "## [3] Downloading wav2vec2 (plain or WithLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:30:35.965936Z",
     "iopub.status.busy": "2024-06-15T19:30:35.965936Z",
     "iopub.status.idle": "2024-06-15T19:30:35.972973Z",
     "shell.execute_reply": "2024-06-15T19:30:35.972973Z",
     "shell.execute_reply.started": "2024-06-15T19:30:35.965936Z"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1718359439487,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "65WVbDBfIgUw",
    "outputId": "16f08152-bfbd-4c0f-d16a-334b4fdaef48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running localy, good luck\n",
      "\n",
      "Using model from path: D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavih-hr\\wav2vec2-large-slavic-parlaspeech-hr\n"
     ]
    }
   ],
   "source": [
    "# Download HF repository manually and place path (public, ~2GB download)\n",
    "import os\n",
    "\n",
    "\n",
    "if google_colabbing == True:\n",
    "    \n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    if choose_lm_model == False:\n",
    "      # Check if the directory exists\n",
    "      if not os.path.exists(\"/content/wav2vec2-large-slavic-parlaspeech-hr\"):\n",
    "          # Clone the repository if it doesn't exist\n",
    "          !git clone https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr\n",
    "          os.makedirs(\"/content/wav2vec2-large-slavic-parlaspeech-hr/temp\", exist_ok=True)\n",
    "      else:\n",
    "          print(f\"wav2vec2-parlaspeech-hr already exists.\")\n",
    "    \n",
    "      model_file_path = \"/content/wav2vec2-large-slavic-parlaspeech-hr\"\n",
    "      print(f\"\\nUsing model from path: {model_file_path}\")\n",
    "    \n",
    "    else:\n",
    "      # Check if the directory exists\n",
    "      if not os.path.exists(\"/content/pytorch/wav2vec2-large-slavic-hr-lm\"):\n",
    "          # Clone the repository if it doesn't exist\n",
    "          !git clone https://huggingface.co/classla/wav2vec2-large-slavic-parlaspeech-hr-lm\n",
    "          os.makedirs(\"/content/wav2vec2-large-slavic-parlaspeech-hr-lm/temp\", exist_ok=True)\n",
    "      else:\n",
    "          print(f\"wav2vec2-parlaspeech-hr-lm already exists.\")\n",
    "    \n",
    "      model_file_path = \"/content/pytorch/wav2vec2-large-slavic-hr-lm\"\n",
    "      print(f\"\\nUsing model from path: {model_file_path}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Running localy, good luck\")\n",
    "    \n",
    "    if choose_lm_model == False:\n",
    "        model_file_path = path_to_pshr_wav2vec2_l\n",
    "    else: \n",
    "        model_file_path = path_to_pshr_wav2vec2_l_lm\n",
    "    print(f\"\\nUsing model from path: {model_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay3Tr6dBR0gn"
   },
   "source": [
    "## [4] Initilize the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:00.389556Z",
     "iopub.status.busy": "2024-06-15T19:31:00.389556Z",
     "iopub.status.idle": "2024-06-15T19:31:00.403631Z",
     "shell.execute_reply": "2024-06-15T19:31:00.403631Z",
     "shell.execute_reply.started": "2024-06-15T19:31:00.389556Z"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1718361529289,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "KbCTLV-yHiZh",
    "outputId": "97943b4f-e296-416a-e5b5-d25330207698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available or GPU is too old.\n",
      "device set to: CPU (cpu)\n",
      "device: cpu\n",
      "tokenizer0 exists.\n",
      "feature_extractor0 exists.\n",
      "processor0 exists.\n",
      "model0 exists. device cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ProcessorWithLM, Wav2Vec2ForCTC, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Config\n",
    "import torch\n",
    "import os\n",
    "import kenlm\n",
    "from pyctcdecode import BeamSearchDecoderCTC, build_ctcdecoder\n",
    "\n",
    "# Set device as gpu, default = cpu\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 3.7:\n",
    "    device = torch.device(\"cuda:0\")  # Use GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use CPU\n",
    "    print(\"CUDA not available or GPU is too old.\")\n",
    "    print(f\"device set to: CPU ({device})\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "\n",
    "config_file_path = os.path.join(model_file_path, \"config.json\")\n",
    "config_json = Wav2Vec2Config.from_json_file(config_file_path)\n",
    "\n",
    "\n",
    "if choose_lm_model == False:\n",
    "  # Make the tokenizer\n",
    "  if 'tokenizer0' not in globals():\n",
    "    print(f\"tokenizer0 not found. Making tokenizer0...\")\n",
    "    tokenizer0 = Wav2Vec2CTCTokenizer(model_file_path + \"/vocab.json\",\n",
    "                                    unk_token=\"[UNK]\",\n",
    "                                    pad_token=\"[PAD]\",\n",
    "                                    word_delimiter_token=\"|\")\n",
    "  else:\n",
    "    print(f\"tokenizer0 exists.\")\n",
    "\n",
    "  # Make feature_extractor manually taken from preprocessor_config.json\n",
    "  if 'feature_extractor0' not in globals():\n",
    "    print(f\"feature_extractor0 not found. Making feature_extractor0...\")\n",
    "    feature_extractor0 = Wav2Vec2FeatureExtractor(feature_size=1,\n",
    "                                                sampling_rate=16000,\n",
    "                                                padding_value=0.0,\n",
    "                                                do_normalize=True,\n",
    "                                                return_attention_mask=True)\n",
    "  else:\n",
    "    print(f\"feature_extractor0 exists.\")\n",
    "\n",
    "  # Make the processor\n",
    "  if 'processor0' not in globals():\n",
    "    print(f\"processor0 not found. Making processor0...\")\n",
    "    processor0 = Wav2Vec2Processor.from_pretrained(model_file_path)\n",
    "  else:\n",
    "      print(\"processor0 exists.\")\n",
    "\n",
    "  # Make the model\n",
    "  if 'model0' not in globals():\n",
    "    print(f\"model0 not found. Making model0... device {device}\")\n",
    "    model0 = Wav2Vec2ForCTC.from_pretrained(model_file_path,\n",
    "                                            config=config_json).to(device)\n",
    "  else:\n",
    "    print(f\"model0 exists. device {device}\")\n",
    "  model0.to(device)\n",
    "  model0.eval()\n",
    "\n",
    "\n",
    "else:\n",
    "  # Make the tokenizer\n",
    "  if 'tokenizer0lm' not in globals():\n",
    "    print(f\"tokenizer0lm not found. Making tokenizer0lm...\")\n",
    "    tokenizer0lm = Wav2Vec2CTCTokenizer(model_file_path + \"/vocab.json\",\n",
    "                                    unk_token=\"[UNK]\",\n",
    "                                    pad_token=\"[PAD]\",\n",
    "                                    word_delimiter_token=\"|\")\n",
    "  else:\n",
    "    print(f\"tokenizer0lm exists.\")\n",
    "\n",
    "  # Make feature_extractor manually taken from preprocessor_config.json\n",
    "  if 'feature_extractor0lm' not in globals():\n",
    "    print(f\"feature_extractor0lm not found. Making feature_extractor0lm...\")\n",
    "    feature_extractor0lm = Wav2Vec2FeatureExtractor(feature_size=1,\n",
    "                                                    sampling_rate=16000,\n",
    "                                                    padding_value=0.0,\n",
    "                                                    do_normalize=True,\n",
    "                                                    return_attention_mask=True)\n",
    "  else:\n",
    "    print(f\"feature_extractor0lm exists.\")\n",
    "\n",
    "  # Make the processor\n",
    "  if 'processor0lm' not in globals():\n",
    "    print(f\"processor0lm not found. Making processor0lm...\")\n",
    "    processor0lm = Wav2Vec2ProcessorWithLM.from_pretrained(model_file_path)\n",
    "  else:\n",
    "    print(\"processor0lm exists.\")\n",
    "\n",
    "  # Make the model\n",
    "  if 'model0lm' not in globals():\n",
    "    print(f\"model0lm not found. Making model0lm...  device {device}\")\n",
    "    model0lm = Wav2Vec2ForCTC.from_pretrained(model_file_path,\n",
    "                                              config=model_file_path + \"/config.json\").to(device)\n",
    "  else:\n",
    "      print(f\"model0lm exists. device {device}\")\n",
    "  model0lm.to(device)\n",
    "  model0lm.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8eTi90-R7E_"
   },
   "source": [
    "## [5] Define cutting down into temp chunks (now with DYNAMICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:02.118672Z",
     "iopub.status.busy": "2024-06-15T19:31:02.117674Z",
     "iopub.status.idle": "2024-06-15T19:31:02.780889Z",
     "shell.execute_reply": "2024-06-15T19:31:02.780889Z",
     "shell.execute_reply.started": "2024-06-15T19:31:02.118672Z"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1718360508968,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "15EPOU2EL7Xu",
    "outputId": "a676d1d2-e0d8-4746-a7d6-d1519087be8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is debug_printing on? False\n",
      "\n",
      "Transcription function defined and active: \n",
      "Splitting audio in: 20s segments (dynamically). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TIME_CHUNK = 20\n",
    "target_sr=16000\n",
    "\n",
    "SILENCE_THRESHOLD = 0.01  # Threshold to detect silence\n",
    "MIN_SILENCE_LENGTH = 0.5  # Minimum length of silence to be considered a split point in seconds\n",
    "\n",
    "\n",
    "print(f\"Is debug_printing on? {debug_printing}\\n\")\n",
    "\n",
    "### PROCESSING ENGINE ###\n",
    "\n",
    "def process_transcribe(input_file):\n",
    "    \n",
    "        # Begin processing:\n",
    "        speech, sample_rate = torchaudio.load(input_file)\n",
    "\n",
    "        # Check if resampling is needed\n",
    "        if sample_rate != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "            speech = resampler(speech)\n",
    "\n",
    "        # Process the speech with the processor\n",
    "        input_values = processor0(speech, sampling_rate=target_sr, return_tensors=\"pt\").input_values.to(device)\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(input_values.shape)\n",
    "\n",
    "        if input_values.dim() == 3:  # If the shape is [1, 1, audio_length]\n",
    "            input_values = input_values.squeeze(0)  # Squeeze to [1, audio_length]\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(input_values.shape)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.inference_mode():\n",
    "            logits = model0(input_values).logits\n",
    "\n",
    "        if debug_printing == True:\n",
    "         print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "        # Get the predicted token IDs (greedy decoding)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(\"Predicted IDs shape:\", predicted_ids.shape)\n",
    "\n",
    "        # Convert predicted IDs to numpy array\n",
    "        predicted_ids = predicted_ids.cpu().numpy()\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(\"Predicted IDs (numpy):\", predicted_ids)\n",
    "\n",
    "        # Decode the predicted IDs\n",
    "        transcription = processor0.batch_decode(predicted_ids)[0]\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(\"Transcription:\", transcription)\n",
    "\n",
    "        return transcription\n",
    "\n",
    "\n",
    "\n",
    "def transcribe(input_file, output_folder, target_sr=target_sr, TIME_CHUNK=TIME_CHUNK):\n",
    "    # Load the audio\n",
    "    waveform, orig_sr = torchaudio.load(input_file)\n",
    "\n",
    "    # Check if resampling is needed\n",
    "    if orig_sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Determine the output file name and format (always WAV)\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    # Convert waveform to numpy array for easier processing\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "\n",
    "\n",
    "################### RUN THE WHOLE AUDIO FILES ##########################\n",
    "    if testing_asr == True:\n",
    "            # Directly process the entire audio without chunking\n",
    "            #output_file = os.path.join(output_folder, f\"{base_name}_whole.wav\")\n",
    "            #torchaudio.save(output_file, waveform, sample_rate=target_sr)\n",
    "            tekst = process_transcribe(input_file)\n",
    "            # print(f\"Transcription for {base_name}: {tekst}\")\n",
    "            print(tekst)\n",
    "            #os.remove(output_file)  # Optionally delete the temporary WAV file\n",
    "    \n",
    "######### ELSE CHUNKS BASED ON TIME_CHUNK INCREMENTS ##################\n",
    "    else:\n",
    "        # Function to detect silence points\n",
    "        def find_silence_points(signal, threshold, min_silence_length, sr):\n",
    "            silence_points = []\n",
    "            min_silence_samples = int(min_silence_length * sr)\n",
    "            is_silence = np.abs(signal) < threshold\n",
    "            silence_length = 0\n",
    "            for i in range(len(is_silence)):\n",
    "                if is_silence[i]:\n",
    "                    silence_length += 1\n",
    "                    if silence_length >= min_silence_samples:\n",
    "                        silence_points.append(i)\n",
    "                else:\n",
    "                    silence_length = 0\n",
    "            return silence_points\n",
    "\n",
    "\n",
    "        # Find silence points in the audio\n",
    "        silence_points = find_silence_points(waveform_np, SILENCE_THRESHOLD, MIN_SILENCE_LENGTH, target_sr)\n",
    "    \n",
    "        if debug_printing == True:\n",
    "    \n",
    "            print(f\"Plotting point where silence ({len(silence_points)} points) has been detected in {input_file}\")\n",
    "            # Plot the waveform and silence points\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.plot(waveform_np, label='Waveform')\n",
    "            plt.scatter(silence_points, waveform_np[silence_points], color='red', marker='x', label='Silence Points')\n",
    "            plt.xlabel('Samples')\n",
    "            plt.ylabel('Amplitude')\n",
    "            plt.title('Waveform with Silence Points')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "        # Split the audio at silence points\n",
    "        chunk_start = 0\n",
    "        for i, point in enumerate(silence_points):\n",
    "            chunk_end = point\n",
    "            if chunk_end - chunk_start >= TIME_CHUNK * target_sr:\n",
    "                chunk = waveform[:, chunk_start:chunk_end]\n",
    "                output_file = os.path.join(output_folder, f\"{base_name}_temp_chunk{i}.wav\")\n",
    "                torchaudio.save(output_file, chunk, sample_rate=target_sr)\n",
    "    \n",
    "                # Process the temporary WAV file\n",
    "                tekst = process_transcribe(output_file)\n",
    "                time = format_time(chunk_start / target_sr)\n",
    "                print(f\"[{time}] {tekst}\")\n",
    "    \n",
    "                # Optionally, you can delete the temporary WAV file after processing\n",
    "                os.remove(output_file)\n",
    "    \n",
    "                chunk_start = chunk_end\n",
    "                \n",
    "        def format_time(seconds):\n",
    "            minutes = int(seconds // 60)\n",
    "            seconds = int(seconds % 60)\n",
    "            return f\"{minutes:02d}:{seconds:02d}\"\n",
    "    \n",
    "        # Process the remaining audio if any\n",
    "        if chunk_start < waveform.size(1):\n",
    "            chunk = waveform[:, chunk_start:]\n",
    "            output_file = os.path.join(output_folder, f\"{base_name}_temp_chunk{len(silence_points)}.wav\")\n",
    "            torchaudio.save(output_file, chunk, sample_rate=target_sr)\n",
    "            tekst = process_transcribe(output_file)\n",
    "            time = format_time(chunk_start / target_sr)\n",
    "            print(f\"[{time}] {tekst}\")\n",
    "            os.remove(output_file)\n",
    "\n",
    "\n",
    "print(f\"Transcription function defined and active: \\nSplitting audio in: {TIME_CHUNK}s segments (dynamically). \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:02.781888Z",
     "iopub.status.busy": "2024-06-15T19:31:02.781888Z",
     "iopub.status.idle": "2024-06-15T19:31:02.794993Z",
     "shell.execute_reply": "2024-06-15T19:31:02.794993Z",
     "shell.execute_reply.started": "2024-06-15T19:31:02.781888Z"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1718360508968,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "15EPOU2EL7Xu",
    "outputId": "a676d1d2-e0d8-4746-a7d6-d1519087be8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is debug_printing on? False\n",
      "\n",
      "Transcription function defined and active: \n",
      "Splitting audio in: 20s segments (dynamically). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TIME_CHUNK = 20\n",
    "target_sr=16000\n",
    "\n",
    "SILENCE_THRESHOLD = 0.01  # Threshold to detect silence\n",
    "MIN_SILENCE_LENGTH = 0.5  # Minimum length of silence to be considered a split point in seconds\n",
    "\n",
    "\n",
    "print(f\"Is debug_printing on? {debug_printing}\\n\")\n",
    "\n",
    "### PROCESSING ENGINE ###\n",
    "\n",
    "def process_transcribe_lm(input_file):\n",
    "\n",
    "        # Begin processing:\n",
    "        speech, sample_rate = torchaudio.load(input_file)\n",
    "\n",
    "        # Check if resampling is needed\n",
    "        if sample_rate != target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "            speech = resampler(speech)\n",
    "\n",
    "        # Process the speech with the processor\n",
    "        input_values = processor0lm(speech, sampling_rate=target_sr, return_tensors=\"pt\").input_values.to(device)\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(input_values.shape)\n",
    "\n",
    "        if input_values.dim() == 3:  # If the shape is [1, 1, audio_length]\n",
    "            input_values = input_values.squeeze(0)  # Squeeze to [1, audio_length]\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(input_values.shape)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.inference_mode():\n",
    "            logits = model0lm(input_values).logits\n",
    "\n",
    "        #logits = logits.view(-1, logits.size(-1))\n",
    "\n",
    "        if debug_printing == True:\n",
    "            print(\"Logits shape:\", logits.shape)\n",
    "            print(\"Logits dtype:\", logits.dtype)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        predicted_ids = torch.argmax(logits, dim=-1).squeeze(0)\n",
    "        #print(predicted_ids)\n",
    "        predicted_ids_list = predicted_ids.tolist()\n",
    "        transcription = processor0.batch_decode([predicted_ids_list])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        # Get the predicted token IDs (greedy decoding)\n",
    "        #predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(\"Predicted IDs shape:\", predicted_ids.shape)\n",
    "\n",
    "        # Convert predicted IDs to numpy array\n",
    "        #predicted_ids = predicted_ids.unsqueeze(0).cpu().numpy()\n",
    "\n",
    "        #predicted_ids = torch.from_numpy(predicted_ids).unsqueeze(0)\n",
    "\n",
    "        if debug_printing == True:\n",
    "          print(\"Predicted IDs (numpy) shape:\", predicted_ids.shape)\n",
    "\n",
    "        # Decode the predicted IDs\n",
    "        #transcription = processor0lm.batch_decode(predicted_ids)[0]\n",
    "        #transcription = processor0lm.batch_decode(logits.unsqueeze(0))[0]\n",
    "    \n",
    "        if debug_printing == True:\n",
    "          print(\"Transcription:\", transcription)\n",
    "\n",
    "        return transcription\n",
    "\n",
    "\n",
    "def transcribe_lm(input_file, output_folder, target_sr=target_sr, TIME_CHUNK=TIME_CHUNK):\n",
    "    # Load the audio\n",
    "    waveform, orig_sr = torchaudio.load(input_file)\n",
    "\n",
    "    # Check if resampling is needed\n",
    "    if orig_sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=orig_sr, new_freq=target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Determine the output file name and format (always WAV)\n",
    "    base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    # Convert waveform to numpy array for easier processing\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "\n",
    "\n",
    "################### RUN THE WHOLE AUDIO FILES ##########################\n",
    "    if testing_asr == True:\n",
    "            # Directly process the entire audio without chunking\n",
    "            #output_file = os.path.join(output_folder, f\"{base_name}_whole.wav\")\n",
    "            #torchaudio.save(output_file, waveform, sample_rate=target_sr)\n",
    "            tekst = process_transcribe_lm(input_file)\n",
    "            # print(f\"Transcription for {base_name}: {tekst}\")\n",
    "            print(tekst)\n",
    "            #os.remove(output_file)  # Optionally delete the temporary WAV file\n",
    "    \n",
    "######### ELSE CHUNKS BASED ON TIME_CHUNK INCREMENTS ##################\n",
    "    else:\n",
    "        # Function to detect silence points\n",
    "        def find_silence_points(signal, threshold, min_silence_length, sr):\n",
    "            silence_points = []\n",
    "            min_silence_samples = int(min_silence_length * sr)\n",
    "            is_silence = np.abs(signal) < threshold\n",
    "            silence_length = 0\n",
    "            for i in range(len(is_silence)):\n",
    "                if is_silence[i]:\n",
    "                    silence_length += 1\n",
    "                    if silence_length >= min_silence_samples:\n",
    "                        silence_points.append(i)\n",
    "                else:\n",
    "                    silence_length = 0\n",
    "            return silence_points\n",
    "\n",
    "\n",
    "        # Find silence points in the audio\n",
    "        silence_points = find_silence_points(waveform_np, SILENCE_THRESHOLD, MIN_SILENCE_LENGTH, target_sr)\n",
    "    \n",
    "        if debug_printing == True:\n",
    "    \n",
    "            print(f\"Plotting point where silence ({len(silence_points)} points) has been detected in {input_file}\")\n",
    "            # Plot the waveform and silence points\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.plot(waveform_np, label='Waveform')\n",
    "            plt.scatter(silence_points, waveform_np[silence_points], color='red', marker='x', label='Silence Points')\n",
    "            plt.xlabel('Samples')\n",
    "            plt.ylabel('Amplitude')\n",
    "            plt.title('Waveform with Silence Points')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "        # Split the audio at silence points\n",
    "        chunk_start = 0\n",
    "        for i, point in enumerate(silence_points):\n",
    "            chunk_end = point\n",
    "            if chunk_end - chunk_start >= TIME_CHUNK * target_sr:\n",
    "                chunk = waveform[:, chunk_start:chunk_end]\n",
    "                output_file = os.path.join(output_folder, f\"{base_name}_temp_chunk{i}.wav\")\n",
    "                torchaudio.save(output_file, chunk, sample_rate=target_sr)\n",
    "    \n",
    "                # Process the temporary WAV file\n",
    "                tekst = process_transcribe_lm(output_file)\n",
    "                time = format_time(chunk_start / target_sr)\n",
    "                print(f\"[{time}] {tekst}\")\n",
    "    \n",
    "                # Optionally, you can delete the temporary WAV file after processing\n",
    "                os.remove(output_file)\n",
    "    \n",
    "                chunk_start = chunk_end\n",
    "                \n",
    "        def format_time(seconds):\n",
    "            minutes = int(seconds // 60)\n",
    "            seconds = int(seconds % 60)\n",
    "            return f\"{minutes:02d}:{seconds:02d}\"\n",
    "    \n",
    "        # Process the remaining audio if any\n",
    "        if chunk_start < waveform.size(1):\n",
    "            chunk = waveform[:, chunk_start:]\n",
    "            output_file = os.path.join(output_folder, f\"{base_name}_temp_chunk{len(silence_points)}.wav\")\n",
    "            torchaudio.save(output_file, chunk, sample_rate=target_sr)\n",
    "            tekst = process_transcribe_lm(output_file)\n",
    "            time = format_time(chunk_start / target_sr)\n",
    "            print(f\"[{time}] {tekst}\")\n",
    "            os.remove(output_file)\n",
    "\n",
    "\n",
    "print(f\"Transcription function defined and active: \\nSplitting audio in: {TIME_CHUNK}s segments (dynamically). \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_ZwYCvBSBtG"
   },
   "source": [
    "## [6] Quick test to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:06.469511Z",
     "iopub.status.busy": "2024-06-15T19:31:06.469511Z",
     "iopub.status.idle": "2024-06-15T19:31:06.471814Z",
     "shell.execute_reply": "2024-06-15T19:31:06.471814Z",
     "shell.execute_reply.started": "2024-06-15T19:31:06.469511Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_pshr_wav2vec2_l = r\"D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavih-hr\\wav2vec2-large-slavic-parlaspeech-hr\"\n",
    "path_to_pshr_wav2vec2_l_lm = r\"D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavic-hr-lm\"\n",
    "\n",
    "#test_audio_path_pc = r\"D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavic-hr-lm\\nela_film_review.ogg\" # WORKS WELL\n",
    "test_audio_path_pc = r\"D:\\ASR\\ParlaspeechHR\\ivan_snimka.opus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:06.972802Z",
     "iopub.status.busy": "2024-06-15T19:31:06.972802Z",
     "iopub.status.idle": "2024-06-15T19:31:13.678152Z",
     "shell.execute_reply": "2024-06-15T19:31:13.677752Z",
     "shell.execute_reply.started": "2024-06-15T19:31:06.972802Z"
    },
    "executionInfo": {
     "elapsed": 27946,
     "status": "ok",
     "timestamp": 1718360448610,
     "user": {
      "displayName": "Ivan Porupski",
      "userId": "03127310887960034948"
     },
     "user_tz": -120
    },
    "id": "qovFsXvCNTEc",
    "outputId": "d50a5b1a-a99b-43aa-d938-6b758934dbd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No chunking. Per file basis. (testing_asr = True)\n",
      "\n",
      "sada ti šaljem glasovnu poruku i ne znam šta pričam ali evo pričam znači ovo tu mi je trebalo jedno 30 sati da dođem do ovog trenutka pričem normlno razgovjetno i nadam se da će transkripcija biti dobra naime ja to nisam izmislio nego su to zapravo r slovensa uzeli od facboka model i strenirali ga na govoru iz parlamenta specifično za hrvatski jeziki\n"
     ]
    }
   ],
   "source": [
    "if testing_asr == True:\n",
    "    print(f\"No chunking. Per file basis. (testing_asr = {testing_asr})\\n\")\n",
    "else:\n",
    "    print(f\"Chunking audio! Using silence points. (testing_asr = {testing_asr})\\n\")\n",
    "\n",
    "\n",
    "if google_colabbing == True:\n",
    "\n",
    "    test_audio_path = \"/content/drive/MyDrive/pytorch/wav2vec2-large-slavic-hr-lm/ivan_snimka.opus\"\n",
    "    test_audio_temp = \"/content/wav2vec2-large-slavic-parlaspeech-hr/temp\"\n",
    "    \n",
    "else:\n",
    "    if choose_lm_model == False:\n",
    "        # Običan model\n",
    "        test_audio_path = test_audio_path_pc\n",
    "        test_audio_temp = path_to_pshr_wav2vec2_l + r\"\\temp\"\n",
    "        transcribe(test_audio_path, test_audio_temp)\n",
    "        \n",
    "    else:\n",
    "        # LM model\n",
    "        test_audio_path = test_audio_path_pc\n",
    "        test_audio_temp = path_to_pshr_wav2vec2_l_lm + r\"\\temp\"\n",
    "        transcribe_lm(test_audio_path, test_audio_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:15.221203Z",
     "iopub.status.busy": "2024-06-15T19:31:15.221203Z",
     "iopub.status.idle": "2024-06-15T19:31:15.224129Z",
     "shell.execute_reply": "2024-06-15T19:31:15.224129Z",
     "shell.execute_reply.started": "2024-06-15T19:31:15.221203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50\n"
     ]
    }
   ],
   "source": [
    "    # Check the vocabulary size of the processor\n",
    "    vocab_size = len(processor0.tokenizer.get_vocab())\n",
    "    print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zENOebskRMRG"
   },
   "source": [
    "# MODEL IS NOW OPERATIONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8dG337mSHjb"
   },
   "source": [
    "## [7] Prepare to run ling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:18.950009Z",
     "iopub.status.busy": "2024-06-15T19:31:18.941684Z",
     "iopub.status.idle": "2024-06-15T19:31:20.839140Z",
     "shell.execute_reply": "2024-06-15T19:31:20.838576Z",
     "shell.execute_reply.started": "2024-06-15T19:31:18.950009Z"
    },
    "id": "uXKBhHqgNS7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files in 'D:\\ParlaSpeech-HR.v1.0\\raw': 403925\n",
      "\n",
      "Number of entries in 'D:\\ParlaSpeech-HR.v1.0\\ParlaSpeech-HR.v1.0.jsonl': 403925\n",
      "\n",
      "CPU times: total: 484 ms\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "# Directory containing audio files\n",
    "path_to_pshr_raw_audio_data = r\"D:\\ParlaSpeech-HR.v1.0\\raw\"\n",
    "\n",
    "# List all files in the directory\n",
    "audio_files = os.listdir(path_to_pshr_raw_audio_data)\n",
    "\n",
    "# Count the number of audio files\n",
    "num_audio_files = len(audio_files)\n",
    "\n",
    "print(f\"Number of audio files in '{path_to_pshr_raw_audio_data}': {num_audio_files}\\n\")\n",
    "\n",
    "# JSONL file containing metadata\n",
    "path_to_pshr_raw_jsonl = r\"D:\\ParlaSpeech-HR.v1.0\\ParlaSpeech-HR.v1.0.jsonl\"\n",
    "\n",
    "# Counting lines in the JSONL file\n",
    "num_entries = 0\n",
    "with open(path_to_pshr_raw_jsonl, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        num_entries += 1\n",
    "\n",
    "print(f\"Number of entries in '{path_to_pshr_raw_jsonl}': {num_entries}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Paths to your data\n",
    "path_to_pshr_raw_audio_data = r\"D:\\ParlaSpeech-HR.v1.0\\raw\"\n",
    "path_to_pshr_raw_jsonl = r\"D:\\ParlaSpeech-HR.v1.0\\ParlaSpeech-HR.v1.0.jsonl\"\n",
    "\n",
    "# Limit number of transcriptions to print\n",
    "limit = 5\n",
    "count = 0\n",
    "\n",
    "# Read JSONL file\n",
    "audio_transcriptions = {}\n",
    "with open(path_to_pshr_raw_jsonl, 'r', encoding='utf-8') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        if count >= limit:\n",
    "            break\n",
    "        \n",
    "        data = json.loads(line.strip())\n",
    "        audio_filename = os.path.basename(data['path'])\n",
    "        audio_transcriptions[audio_filename] = data['norm_words']  # Use 'norm_words' or other transcription field\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "audio_transcriptions = dict(sorted(audio_transcriptions.items()))\n",
    "\n",
    "print(f\"audio_transcriptions: {audio_transcriptions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generiranje audio_transcriptions dict(sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:30.725404Z",
     "iopub.status.busy": "2024-06-15T19:31:30.725404Z",
     "iopub.status.idle": "2024-06-15T19:31:42.562851Z",
     "shell.execute_reply": "2024-06-15T19:31:42.562851Z",
     "shell.execute_reply.started": "2024-06-15T19:31:30.725404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL AUDIO_TRANSCRIPTIONS HERE\n",
      "CPU times: total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"FULL AUDIO_TRANSCRIPTIONS HERE\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Paths to your data\n",
    "path_to_pshr_raw_audio_data = r\"D:\\ParlaSpeech-HR.v1.0\\raw\"\n",
    "path_to_pshr_raw_jsonl = r\"D:\\ParlaSpeech-HR.v1.0\\ParlaSpeech-HR.v1.0.jsonl\"\n",
    "\n",
    "# Read JSONL file\n",
    "audio_transcriptions = {}\n",
    "with open(path_to_pshr_raw_jsonl, 'r', encoding='utf-8') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line.strip())\n",
    "        audio_filename = os.path.basename(data['path'])\n",
    "        audio_transcriptions[audio_filename] = data['norm_words']  # Use 'norm_words' or other transcription field\n",
    "\n",
    "audio_transcriptions = dict(sorted(audio_transcriptions.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:44.262366Z",
     "iopub.status.busy": "2024-06-15T19:31:44.261374Z",
     "iopub.status.idle": "2024-06-15T19:31:44.427173Z",
     "shell.execute_reply": "2024-06-15T19:31:44.427173Z",
     "shell.execute_reply.started": "2024-06-15T19:31:44.262366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying first 3 audio names w/ transcription:\n",
      "\n",
      "debug_printing: False\n",
      "\n",
      "Correlating audio files with transcriptions:\n",
      "\n",
      "Audio File:\n",
      "seg.-EO06cT21uY_10038.21-10054.64.flac\n",
      "\n",
      "Transcription:\n",
      "lijepo kolegice i kolege nemojte se vi varda hihotat prije četirig ste izdali birače sad jedva čekam da vidim kako će vas nagradit jedva čekam hvala lijepo kolegice i kolega kolege zastupnici g potpredsjedniče\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Audio File:\n",
      "seg.-EO06cT21uY_10111.7-10131.55.flac\n",
      "\n",
      "Transcription:\n",
      "hadeze želi pokušati dati obećanje za šest mjeseci da će neki građani dobiti osam stotina kuna i jedina svrha ovog zakona su je kampanja odnosno izbori i predizborna kampanja jer kako drugačije objasniti da niste dali kada\n",
      "------------------------------\n",
      "\n",
      "\n",
      "Audio File:\n",
      "seg.-EO06cT21uY_10131.55-10147.16.flac\n",
      "\n",
      "Transcription:\n",
      "ste imali rast tri posto a sada predlaže kao fol g aladrović i plenković se brinu za naše starije sugrađane i evo kad ćemo imati pad od deset posto izdvojili su osamsto kuna\n",
      "------------------------------\n",
      "\n",
      "\n",
      "CPU times: total: 156 ms\n",
      "Wall time: 157 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Print limited number of audio_transcriptions\n",
    "limit = 3\n",
    "count = 0\n",
    "\n",
    "print(f\"Displaying first {limit} audio names w/ transcription:\\n\")\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(path_to_pshr_raw_audio_data)\n",
    "\n",
    "# Sort the files alphabetically or by any other criteria, if needed\n",
    "all_files.sort()\n",
    "\n",
    "# Select the first 10 files\n",
    "first_10_files = all_files[:10]\n",
    "\n",
    "print(f\"debug_printing: {debug_printing}\")\n",
    "\n",
    "if debug_printing == True:\n",
    "    # Debug prints\n",
    "    print(\"First 10 Files:\")\n",
    "    print(first_10_files)\n",
    "    print(\"\\nAudio Transcriptions:\")\n",
    "\n",
    "for filename, transcription in audio_transcriptions.items():\n",
    "    if count >= limit:\n",
    "        break\n",
    "    if debug_printing == True:\n",
    "        print(f\"{filename}: {transcription}\")\n",
    "    count += 1\n",
    "\n",
    "# Step 3: Correlate audio files with transcriptions (limiting to 'limit' iterations)\n",
    "print(\"\\nCorrelating audio files with transcriptions:\\n\")\n",
    "for i, audio_file in enumerate(first_10_files[:limit]):\n",
    "    if audio_file in audio_transcriptions:\n",
    "        transcription = ' '.join(audio_transcriptions[audio_file])\n",
    "        print(f\"Audio File:\\n{audio_file}\\n\\nTranscription:\\n{transcription}\")\n",
    "    else:\n",
    "        print(f\"No transcription found for {audio_file}\")\n",
    "    \n",
    "    # Add a separator between iterations for clarity, except after the last item\n",
    "    if i < (limit):\n",
    "        print(\"-\" * 30)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING TRANSCRIPTIONS now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:31:48.477895Z",
     "iopub.status.busy": "2024-06-15T19:31:48.477895Z",
     "iopub.status.idle": "2024-06-15T19:32:04.417196Z",
     "shell.execute_reply": "2024-06-15T19:32:04.417196Z",
     "shell.execute_reply.started": "2024-06-15T19:31:48.477895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvala lijepo kolegice i kolege nemojte se vi varda hihotat prije 4 godine ste izdali birače sada jedva čekam da vidim kako će vas nagraditi jedva čekam hvala lijepo kolegice i kolege zastupnici gospodine potpredsjedniče\n",
      "------------------------------\n",
      "hdz želi pokušati dati obećanje za 6 mjesci da će neki građani dobiti 8000 kuna i jedina svrha ovog zakona su je kampanja odnosno izbori i predizborna kampanja jer kako drugačije objasniti da niste dali kada\n",
      "------------------------------\n",
      "ste imali rast 3 a sada predlažete kao hol gospodina ladrović i plenkovića se brinu za naše starije sugrađane i evo kada ćemo imati pad ud 10 izdvojili su 800 kuna\n",
      "------------------------------\n",
      "ma dajte molim vas to nitko ne vjeruje to ne vjeruje nitito ne vjeruju niti djeca koja su nažalost danas doma i gledaju njih 98 školu na televiziji i bilo bi dobro da vlada rh je u stanju organizirati obrazovni sustav da\n",
      "------------------------------\n",
      "djeca mogu stjecati znanje i obrazovati se da pokažu da nas mogu vratiti u normalni period nego što jedino razmišljaju o kampanji izborima i kako osigurati plenkoviću još jedan mandat svi su to prokužili svi to razumiju\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the first 5 .flac files\n",
    "for i, audio_file in enumerate(all_files[:5]):\n",
    "    if audio_file.endswith('.flac'):\n",
    "        flac_audio_path = os.path.join(path_to_pshr_raw_audio_data, audio_file)\n",
    "        transcribe(flac_audio_path, test_audio_temp)\n",
    "    else:\n",
    "        print(f\"Skipping non-FLAC file: {audio_file}\")\n",
    "\n",
    "    # Add a separator between iterations for clarity\n",
    "    if i < 4:  # Adjust the limit based on the number of files you want to process\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:33:40.669377Z",
     "iopub.status.busy": "2024-06-15T19:33:40.669377Z",
     "iopub.status.idle": "2024-06-15T19:33:40.678305Z",
     "shell.execute_reply": "2024-06-15T19:33:40.678305Z",
     "shell.execute_reply.started": "2024-06-15T19:33:40.669377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions online.\n"
     ]
    }
   ],
   "source": [
    "## NEW\n",
    "\n",
    "import os\n",
    "from jiwer import wer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "similarity_threshold = 50  # Example threshold, can be adjusted based on requirements\n",
    "max_search_distance = 3  # Maximum number of tokens to search ahead for a match\n",
    "\n",
    "# Function to tokenize text by removing dots and commas, then splitting into tokens\n",
    "def tokenize_text(text):\n",
    "    # Remove dots and commas\n",
    "    text = text.replace('.', '').replace(',', '')\n",
    "    # Convert to lowercase and split into tokens\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# Function to normalize numbers within tokens\n",
    "def normalize_numbers(tokens, norm_nums):\n",
    "    def decompose_number(num):\n",
    "        components = []\n",
    "        if num >= 1000:\n",
    "            thousands = num // 1000 * 1000\n",
    "            components.append(thousands)\n",
    "            num %= 1000\n",
    "        if num >= 100:\n",
    "            hundreds = num // 100 * 100\n",
    "            components.append(hundreds)\n",
    "            num %= 100\n",
    "        if num >= 20:\n",
    "            tens = num // 10 * 10\n",
    "            components.append(tens)\n",
    "            num %= 10\n",
    "        if num > 0:\n",
    "            components.append(num)\n",
    "        return components\n",
    "\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isdigit():\n",
    "            num = int(token)\n",
    "            components = decompose_number(num)\n",
    "            for component in components:\n",
    "                if component in norm_nums:\n",
    "                    normalized_tokens.append(norm_nums[component])\n",
    "        elif token.lower() in norm_nums:\n",
    "            normalized_tokens.append(norm_nums[token.lower()])\n",
    "        else:\n",
    "            normalized_tokens.append(token)\n",
    "    \n",
    "    # Flatten the list in case there are combined tokens like \"dvije tisuće\"\n",
    "    flattened_tokens = []\n",
    "    for token in normalized_tokens:\n",
    "        if ' ' in token:\n",
    "            flattened_tokens.extend(token.split())\n",
    "        else:\n",
    "            flattened_tokens.append(token)\n",
    "    \n",
    "    return flattened_tokens\n",
    "    \n",
    "\n",
    "\n",
    "# OLD BUT GOOD Function to normalize numbers based on the predefined mapping\n",
    "# def normalize_numbers(tokens, norm_nums):\n",
    "#     normalized_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if token.lower() in norm_nums:\n",
    "#             normalized_tokens.append(norm_nums[token.lower()])\n",
    "#         elif token.isdigit() and int(token) in norm_nums:\n",
    "#             normalized_tokens.append(norm_nums[int(token)])\n",
    "#         else:\n",
    "#             normalized_tokens.append(token)\n",
    "#     return normalized_tokens\n",
    "\n",
    "# Function to align ASR tokens with original tokens based on character similarity\n",
    "def align_tokens(original_tokens, asr_tokens, similarity_threshold=similarity_threshold):\n",
    "    aligned_asr_tokens = []\n",
    "    asr_index = 0\n",
    "    \n",
    "    for original_token in original_tokens:\n",
    "        found_match = False\n",
    "        search_end = min(asr_index + max_search_distance, len(asr_tokens))\n",
    "        for i in range(asr_index, search_end):\n",
    "            asr_token = asr_tokens[i]\n",
    "            similarity = fuzz.ratio(original_token, asr_token)\n",
    "            \n",
    "            if debug_printing == True:\n",
    "                print(f\"Comparing original token '{original_token}' with ASR token '{asr_token}': Similarity = {similarity}\")\n",
    "                \n",
    "            if similarity >= similarity_threshold:\n",
    "                # Found a match, so align both original and ASR tokens\n",
    "                aligned_asr_tokens.append(asr_token)\n",
    "                asr_index = i + 1  # Move the ASR index forward\n",
    "                found_match = True\n",
    "                \n",
    "                if debug_printing == True:\n",
    "                    print(f\"Match found for '{original_token}' with '{asr_token}'\")\n",
    "                    \n",
    "                break\n",
    "        \n",
    "        if not found_match:\n",
    "            # No match found within the search distance, handle as needed (optional)\n",
    "            # For simplicity, you can decide to skip or handle cases where no match is found\n",
    "            if debug_printing == True:\n",
    "                print(f\"No match found for '{original_token}', handling case...\")\n",
    "    \n",
    "    if debug_printing == True:\n",
    "        print(f\"Aligned ASR tokens: {aligned_asr_tokens}\\n\")\n",
    "        \n",
    "    return aligned_asr_tokens\n",
    "\n",
    "print(f\"Functions online.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiranje znamenki (robustnost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:34:01.892715Z",
     "iopub.status.busy": "2024-06-15T19:34:01.892715Z",
     "iopub.status.idle": "2024-06-15T19:34:01.897928Z",
     "shell.execute_reply": "2024-06-15T19:34:01.897928Z",
     "shell.execute_reply.started": "2024-06-15T19:34:01.892715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the mappings for numbers in Croatian\n",
    "norm_nums = {\n",
    "    1: 'jedan',\n",
    "    2: 'dva',\n",
    "    3: 'tri',\n",
    "    4: 'četiri',\n",
    "    5: 'pet',\n",
    "    6: 'šest',\n",
    "    7: 'sedam',\n",
    "    8: 'osam',\n",
    "    9: 'devet',\n",
    "    10: 'deset',\n",
    "    11: 'jedanaest',\n",
    "    12: 'dvanaest',\n",
    "    13: 'trinaest',\n",
    "    14: 'četrnaest',\n",
    "    15: 'petnaest',\n",
    "    16: 'šesnaest',\n",
    "    17: 'sedamnaest',\n",
    "    18: 'osamnaest',\n",
    "    19: 'devetnaest',\n",
    "    20: 'dvadeset',\n",
    "    30: 'trideset',\n",
    "    40: 'četrdeset',\n",
    "    50: 'pedeset',\n",
    "    60: 'šezdeset',\n",
    "    70: 'sedamdeset',\n",
    "    80: 'osamdeset',\n",
    "    90: 'devedeset',\n",
    "    100: 'sto',\n",
    "    200: 'dvjesto',\n",
    "    300: 'tristo',\n",
    "    400: 'četiristo',\n",
    "    500: 'petsto',\n",
    "    600: 'šesto',\n",
    "    700: 'sedamsto',\n",
    "    800: 'osamsto',\n",
    "    900: 'devetsto',\n",
    "    1000: 'tisuća',\n",
    "    2000: 'dvije tisuće',  # Two thousand\n",
    "    3000: 'tri tisuće',    # Three thousand\n",
    "    4000: 'četiri tisuće', # Four thousand\n",
    "    5000: 'pet tisuća',    # Five thousand\n",
    "    6000: 'šest tisuća',   # Six thousand\n",
    "    7000: 'sedam tisuća',  # Seven thousand\n",
    "    8000: 'osam tisuća',   # Eight thousand\n",
    "    9000: 'devet tisuća',  # Nine thousand\n",
    "}\n",
    "\n",
    "# Create the reverse mapping for Croatian numbers\n",
    "norm_nums_reverse = {value: key for key, value in norm_nums.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T19:34:07.512556Z",
     "iopub.status.busy": "2024-06-15T19:34:07.512556Z",
     "iopub.status.idle": "2024-06-15T19:34:07.520482Z",
     "shell.execute_reply": "2024-06-15T19:34:07.520482Z",
     "shell.execute_reply.started": "2024-06-15T19:34:07.512556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['ovo', 'je', '2012', 'godina', 'zar', 'ne']\n",
      "Normalized Tokens: ['ovo', 'je', 'dvije', 'tisuće', 'dvanaest', 'godina', 'zar', 'ne']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tekst = \"Ovo je 2012. godina, zar ne\"\n",
    "\n",
    "asr_tokens = tokenize_text(tekst)\n",
    "print(\"Tokens:\", asr_tokens)\n",
    "\n",
    "normalized_tokens = normalize_numbers(asr_tokens, norm_nums)\n",
    "normalized_tokens = normalized_tokens\n",
    "print(\"Normalized Tokens:\", normalized_tokens)\n",
    "\n",
    "# Expected output\n",
    "# Tokens: ['ovo', 'je', '2012', 'godina', 'zar', 'ne']\n",
    "# Normalized Tokens: ['ovo', 'je', 'dvije', 'tisuće', 'dvanaest', 'godina', 'zar', 'ne']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run WER matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T21:59:54.935374Z",
     "iopub.status.busy": "2024-06-15T21:59:54.935374Z",
     "iopub.status.idle": "2024-06-15T21:59:54.939539Z",
     "shell.execute_reply": "2024-06-15T21:59:54.939539Z",
     "shell.execute_reply.started": "2024-06-15T21:59:54.935374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No WER_results Dict found.\n",
      "Making a new empty one...\n"
     ]
    }
   ],
   "source": [
    "debug_printing = False\n",
    "WER_results = []\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "if choose_lm_model == False:\n",
    "    dict_file_path = path_to_pshr_wav2vec2_l\n",
    "else: \n",
    "    dict_file_path = path_to_pshr_wav2vec2_l_lm\n",
    "\n",
    "\n",
    "dict_main_name = f\"WER_results_{work_version}.json\"\n",
    "\n",
    "dict_file_path_full = os.path.join(dict_file_path, dict_main_name)\n",
    "\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(dict_file_path_full):\n",
    "    \n",
    "    print(f\"Loading WER_results...\")\n",
    "    # Load existing data from JSON file\n",
    "    with open(dict_file_path_full, \"r\") as f:\n",
    "        WER_results = json.load(f)\n",
    "    print(f\"Loaded WER_results from: {dict_file_path_full}\")\n",
    "    \n",
    "else:\n",
    "    # Initialize WER_results as an empty list\n",
    "    WER_results = []\n",
    "    print(f\"No WER_results Dict found.\\nMaking a new empty one...\")\n",
    "\n",
    "\n",
    "# with open(dict_file_path_full, \"w\") as f:\n",
    "#     json.dump(WER_results, f, indent=4)\n",
    "\n",
    "# print(f\"Saving dict to: {dict_file_path}\\  \\n\\nFilename: {dict_main_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T22:18:51.736690Z",
     "iopub.status.busy": "2024-06-15T22:18:51.735692Z",
     "iopub.status.idle": "2024-06-15T22:39:57.855331Z",
     "shell.execute_reply": "2024-06-15T22:39:57.855331Z",
     "shell.execute_reply.started": "2024-06-15T22:18:51.736690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:03<00:00,  3.09s/it]\n",
      "Run 2: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:03<00:00,  3.10s/it]\n",
      "Run 3: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:11<00:00,  3.28s/it]\n",
      "Run 4: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:11<00:00,  3.29s/it]\n",
      "Run 5: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:12<00:00,  3.32s/it]\n",
      "Run 6: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:05<00:00,  3.13s/it]\n",
      "Run 7: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [01:57<00:00,  2.93s/it]\n",
      "Run 8: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:10<00:00,  3.26s/it]\n",
      "Run 9: 100%|███████████████████████████████████████████████████████████████████████████| 40/40 [02:09<00:00,  3.24s/it]\n",
      "Run 10: 100%|██████████████████████████████████████████████████████████████████████████| 40/40 [01:56<00:00,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved dict to: D:\\ASR\\ParlaspeechHR\\wav2vec2-large-slavih-hr\\wav2vec2-large-slavic-parlaspeech-hr\\  \n",
      "\n",
      "Filename: WER_results_v3.2.json\n",
      "\n",
      "Number of runs: 10\n",
      "Total number of audio processed: 400\n",
      "\n",
      "Number of WER_results: 725\n",
      "\n",
      "CPU times: total: 3h 17min 4s\n",
      "Wall time: 21min 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# treba 4s po jednom audio fileu\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Variables to accumulate metrics\n",
    "num_runs = 10  # Number of runs to average over (SAVES DICT EACH RUN)\n",
    "max_files_to_process = 40  # Number of files to process per run (20 files = 1 min)\n",
    "\n",
    "transcription_list = list(audio_transcriptions.items())\n",
    "\n",
    "# Iterate over each run\n",
    "for run in range(num_runs):\n",
    "    total_asr_deletions = 0\n",
    "    total_asr_additions = 0\n",
    "    total_asr_substitutions = 0  # Total substitutions across all files\n",
    "    total_tokens = 0\n",
    "    total_error_rate = 0.0\n",
    "    n = 0  # Number of files processed\n",
    "    \n",
    "    # Shuffle the transcription list for each run if needed\n",
    "    if shuffle_transcript == True:\n",
    "        random.shuffle(transcription_list)\n",
    "    \n",
    "    # Use tqdm to show progress for processing each file in the run\n",
    "    with tqdm(total=max_files_to_process, desc=f\"Run {run+1}\") as pbar:\n",
    "        # Process up to max_files_to_process files for this run\n",
    "        run_results = []\n",
    "        for file_idx in range(min(max_files_to_process, len(transcription_list))):  # Ensure we don't exceed the list length\n",
    "            input_file, transcription = transcription_list[file_idx]\n",
    "\n",
    "            # Process transcription from ASR system\n",
    "            tekst = process_transcribe(os.path.join(path_to_pshr_raw_audio_data, input_file))\n",
    "            \n",
    "            # Get original transcription from dictionary\n",
    "            original_transcription = ' '.join(audio_transcriptions[input_file])\n",
    "            \n",
    "            # Tokenize both transcriptions\n",
    "            original_tokens = tokenize_text(original_transcription)\n",
    "            asr_tokens = tokenize_text(tekst)\n",
    "        \n",
    "            # Normalize numbers in ASR tokens\n",
    "            normalized_asr_tokens = normalize_numbers(asr_tokens, norm_nums)\n",
    "        \n",
    "            # Align ASR tokens with original tokens based on character similarity\n",
    "            aligned_asr_tokens = align_tokens(original_tokens, normalized_asr_tokens, similarity_threshold=similarity_threshold)\n",
    "        \n",
    "            # Calculate error metrics\n",
    "            asr_deletion = len(original_tokens) - len(aligned_asr_tokens)\n",
    "            asr_addition = len(normalized_asr_tokens) - len(aligned_asr_tokens)\n",
    "            \n",
    "            # Calculate substitutions between original and aligned ASR tokens\n",
    "            substitutions = 0\n",
    "            for orig_token, asr_token in zip(original_tokens, aligned_asr_tokens):\n",
    "                if orig_token != asr_token:\n",
    "                    substitutions += 1\n",
    "            \n",
    "            total_asr_substitutions += substitutions\n",
    "            total_tokens = len(original_tokens)\n",
    "            error_rate = (asr_deletion + asr_addition + substitutions) / total_tokens\n",
    "        \n",
    "            # Accumulate metrics\n",
    "            total_asr_deletions += asr_deletion\n",
    "            total_asr_additions += asr_addition\n",
    "            total_error_rate += error_rate\n",
    "            n += 1\n",
    "\n",
    "            # Store individual file results for this run including substitutions\n",
    "            file_result = {\n",
    "                \"input_file\": input_file,\n",
    "                \"tokens\": total_tokens,\n",
    "                \"asr_deletions\": asr_deletion,\n",
    "                \"asr_additions\": asr_addition,\n",
    "                \"asr_substitutions\": substitutions,\n",
    "                \"error_rate\": error_rate\n",
    "            }\n",
    "\n",
    "            WER_results.append(file_result)  # Append to WER_results\n",
    "        \n",
    "            # Update tqdm progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "            # Print or use the error metric for each file as needed\n",
    "            if debug_printing == True:\n",
    "                print(f\"Transcription {input_file}: {tekst}\")\n",
    "                print(f\"original_tokens: \\n{original_tokens}\\n\")\n",
    "                print(f\"asr_tokens: \\n{asr_tokens}\\n\")\n",
    "                print(f\"normalized_asr_tokens: \\n{normalized_asr_tokens}\\n\")\n",
    "                print(f\"aligned_asr_tokens: \\n{aligned_asr_tokens}\\n\")\n",
    "                print(f\"ASR Deletions: {asr_deletion}\")\n",
    "                print(f\"ASR Additions: {asr_addition}\")\n",
    "                print(f\"ASR Substitutions: {substitutions}\")\n",
    "                print(f\"Total Tokens: {len(original_tokens)}\")\n",
    "                print(f\"Error Rate (WER): {error_rate}\\n\")\n",
    "\n",
    "      \n",
    "        # Calculate average error metrics for this run\n",
    "        average_asr_deletions = total_asr_deletions / n\n",
    "        average_asr_additions = total_asr_additions / n\n",
    "        average_asr_substitutions = total_asr_substitutions / n\n",
    "        average_error_rate = total_error_rate / n\n",
    "     \n",
    "        # Store average results for this run\n",
    "        run_result = {\n",
    "            \"average_asr_deletions\": average_asr_deletions,\n",
    "            \"average_asr_additions\": average_asr_additions,\n",
    "            \"average_asr_substitutions\": average_asr_substitutions,\n",
    "            \"average_error_rate\": average_error_rate\n",
    "        }\n",
    "    \n",
    "        # Append run_results to results list for this run\n",
    "        WER_results.append(run_result)\n",
    "\n",
    "        with open(dict_file_path_full, \"w\") as f:\n",
    "            json.dump(WER_results, f, indent=4)\n",
    "            \n",
    "        if debug_printing == True:\n",
    "            print(f\"Saving dict to: {dict_file_path}\\  \\n\\nFilename: {dict_main_name}\\n\")\n",
    "\n",
    "\n",
    "if debug_printing == True:\n",
    "    # Print or process results as needed\n",
    "    for run_idx, run_result in enumerate(WER_results):\n",
    "        print(f\"Run {run_idx + 1} Results:\")\n",
    "        for file_result in run_result:\n",
    "            print(f\"Input File: {file_result['input_file']}\")\n",
    "            print(f\"ASR Deletions: {file_result['asr_deletions']}\")\n",
    "            print(f\"ASR Additions: {file_result['asr_additions']}\")\n",
    "            print(f\"Error Rate (WER): {file_result['error_rate']}\")\n",
    "        print()\n",
    "\n",
    "print(f\"\\nSaved dict to: {dict_file_path}\\  \\n\\nFilename: {dict_main_name}\\n\")\n",
    "\n",
    "# Print number of runs\n",
    "print(f\"Number of runs: {num_runs}\")\n",
    "print(f\"Total number of audio processed: {max_files_to_process*num_runs}\\n\")\n",
    "print(f\"Number of WER_results: {len(WER_results)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T22:41:06.647066Z",
     "iopub.status.busy": "2024-06-15T22:41:06.646077Z",
     "iopub.status.idle": "2024-06-15T22:41:06.653270Z",
     "shell.execute_reply": "2024-06-15T22:41:06.653270Z",
     "shell.execute_reply.started": "2024-06-15T22:41:06.646077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Total ASR Deletions: 1041\n",
      "Overall Total ASR Additions: 723\n",
      "Overall Total ASR Substitutions: 8064\n",
      "Overall Total Original Tokens: 25150\n",
      "\n",
      "Overall Word Error Rate (WER): 0.3908\n",
      "\n",
      "Number of individual files processed: 725\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall totals across all runs\n",
    "overall_total_asr_deletions = sum(result.get(\"asr_deletions\", 0) for result in WER_results if isinstance(result, dict))\n",
    "overall_total_asr_additions = sum(result.get(\"asr_additions\", 0) for result in WER_results if isinstance(result, dict))\n",
    "overall_total_asr_substitutions = sum(result.get(\"asr_substitutions\", 0) for result in WER_results if isinstance(result, dict))\n",
    "overall_total_tokens = sum(result.get(\"tokens\", 0) for result in WER_results if isinstance(result, dict))\n",
    "\n",
    "# Calculate overall error rate\n",
    "total_files_with_error_rate = sum(1 for result in WER_results if isinstance(result, dict) and \"error_rate\" in result)\n",
    "overall_total_error_rate = sum(result[\"error_rate\"] for result in WER_results if isinstance(result, dict) and \"error_rate\" in result) / total_files_with_error_rate if total_files_with_error_rate > 0 else 0\n",
    "\n",
    "# Print overall totals including substitutions\n",
    "print(f\"Overall Total ASR Deletions: {overall_total_asr_deletions}\")\n",
    "print(f\"Overall Total ASR Additions: {overall_total_asr_additions}\")\n",
    "print(f\"Overall Total ASR Substitutions: {overall_total_asr_substitutions}\")\n",
    "print(f\"Overall Total Original Tokens: {overall_total_tokens}\\n\")\n",
    "\n",
    "# Calculate WER\n",
    "wer = (overall_total_asr_deletions + overall_total_asr_additions + overall_total_asr_substitutions) / overall_total_tokens\n",
    "\n",
    "# Print WER\n",
    "print(f\"Overall Word Error Rate (WER): {wer:.4f}\\n\")\n",
    "\n",
    "# Print overall average error metrics\n",
    "# print(f\"Overall Average ASR Deletions: {overall_total_asr_deletions / num_files_processed:.4f}\")\n",
    "# print(f\"Overall Average ASR Additions: {overall_total_asr_additions / num_files_processed:.4f}\")\n",
    "# print(f\"Overall Average ASR Substitutions: {overall_total_asr_substitutions / num_files_processed:.4f}\")\n",
    "# print(f\"Overall Average Error Rate (WER): {overall_total_error_rate:.4f}\\n\")\n",
    "\n",
    "# Print number of individual files processed\n",
    "num_files_processed = len([result for result in WER_results if isinstance(result, dict)])\n",
    "print(f\"Number of individual files processed: {num_files_processed}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WER_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run WER matchin BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T20:28:31.901594Z",
     "iopub.status.busy": "2024-06-15T20:28:31.901594Z",
     "iopub.status.idle": "2024-06-15T20:28:31.904491Z",
     "shell.execute_reply": "2024-06-15T20:28:31.904491Z",
     "shell.execute_reply.started": "2024-06-15T20:28:31.901594Z"
    }
   },
   "outputs": [],
   "source": [
    "debug_printing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T20:28:31.905486Z",
     "iopub.status.busy": "2024-06-15T20:28:31.905486Z",
     "iopub.status.idle": "2024-06-15T20:28:31.915337Z",
     "shell.execute_reply": "2024-06-15T20:28:31.915337Z",
     "shell.execute_reply.started": "2024-06-15T20:28:31.905486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching is online.\n"
     ]
    }
   ],
   "source": [
    "### DEFINING BATCH PROCESSING ####\n",
    "\n",
    "def process_transcribe_batch(input_files, target_sr=16000, device=device, debug_printing=debug_printing):\n",
    "    all_speech = []\n",
    "    all_sample_rates = []\n",
    "\n",
    "    # Load and resample all audio files\n",
    "    for input_file in input_files:\n",
    "        speech, sample_rate = torchaudio.load(input_file)\n",
    "        all_sample_rates.append(sample_rate)\n",
    "        if sample_rate != target_sr:\n",
    "            resampler = Resample(orig_freq=sample_rate, new_freq=target_sr)\n",
    "            speech = resampler(speech)\n",
    "        all_speech.append(speech)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    max_length = max(speech.shape[1] for speech in all_speech)\n",
    "\n",
    "    padded_speech = [torch.nn.functional.pad(s, (0, max_length - s.shape[1])) for s in all_speech]\n",
    "\n",
    "    # Stack all padded speech into a single tensor\n",
    "    input_values = torch.stack(padded_speech).squeeze(1).to(device)\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"input_values.shape:\", input_values.shape)\n",
    "\n",
    "    # Process the batch with the processor\n",
    "    input_values = processor0(input_values, sampling_rate=target_sr, return_tensors=\"pt\").input_values.to(device)\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"input_values.shape after processor:\", input_values.shape)\n",
    "\n",
    "    # Fix batches shape\n",
    "    input_values = input_values.squeeze(0)\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"input_values.shape after remix:\", input_values.shape)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.inference_mode():\n",
    "        logits = model0(input_values).logits\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "    # Get the predicted token IDs (greedy decoding)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"Predicted IDs shape:\", predicted_ids.shape)\n",
    "\n",
    "    # Convert predicted IDs to numpy array\n",
    "    predicted_ids = predicted_ids.cpu().numpy()\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"Predicted IDs (numpy):\", predicted_ids)\n",
    "\n",
    "    # Decode the predicted IDs\n",
    "    transcriptions = processor0.batch_decode(predicted_ids)\n",
    "\n",
    "    if debug_printing:\n",
    "        print(\"Transcriptions:\", transcriptions)\n",
    "\n",
    "    return transcriptions\n",
    "\n",
    "print(f\"Batching is online.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T20:31:23.205333Z",
     "iopub.status.busy": "2024-06-15T20:31:23.204335Z",
     "iopub.status.idle": "2024-06-15T20:31:26.580832Z",
     "shell.execute_reply": "2024-06-15T20:31:26.580832Z",
     "shell.execute_reply.started": "2024-06-15T20:31:23.205333Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch:   0%|                                                                     | 0/403925 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:30\u001b[0m\n",
      "Cell \u001b[1;32mIn[60], line 41\u001b[0m, in \u001b[0;36mprocess_transcribe_batch\u001b[1;34m(input_files, target_sr, device, debug_printing)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 41\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model0(input_values)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_printing:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1962\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;124;03m    Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;124;03m    config.vocab_size - 1]`.\u001b[39;00m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1960\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1962\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwav2vec2(\n\u001b[0;32m   1963\u001b[0m     input_values,\n\u001b[0;32m   1964\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1965\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1966\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1967\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1968\u001b[0m )\n\u001b[0;32m   1970\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1971\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1561\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[0;32m   1557\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[0;32m   1558\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[0;32m   1559\u001b[0m )\n\u001b[1;32m-> 1561\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1562\u001b[0m     hidden_states,\n\u001b[0;32m   1563\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1564\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1565\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1566\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1567\u001b[0m )\n\u001b[0;32m   1569\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:893\u001b[0m, in \u001b[0;36mWav2Vec2EncoderStableLayerNorm.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    886\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    887\u001b[0m             layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    888\u001b[0m             hidden_states,\n\u001b[0;32m    889\u001b[0m             attention_mask,\n\u001b[0;32m    890\u001b[0m             output_attentions,\n\u001b[0;32m    891\u001b[0m         )\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 893\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m layer(\n\u001b[0;32m    894\u001b[0m             hidden_states, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[0;32m    895\u001b[0m         )\n\u001b[0;32m    896\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:734\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayerStableLayerNorm.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    732\u001b[0m attn_residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    733\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 734\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    735\u001b[0m     hidden_states, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions\n\u001b[0;32m    736\u001b[0m )\n\u001b[0;32m    737\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    738\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:629\u001b[0m, in \u001b[0;36mWav2Vec2Attention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    625\u001b[0m     attn_weights_reshaped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    627\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m--> 629\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(attn_probs, value_states)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attn_output` should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_output\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    635\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Variables to accumulate metrics\n",
    "num_runs = 2  # Number of runs to average over, each time picking anew randoms\n",
    "batch_size = 1  # Number of files to process per batch\n",
    "\n",
    "transcription_list = list(audio_transcriptions.items())  # Assume audio_transcriptions is defined somewhere\n",
    "\n",
    "# Iterate over each run\n",
    "for run in range(num_runs):\n",
    "    total_asr_deletions = 0\n",
    "    total_asr_additions = 0\n",
    "    total_tokens = 0\n",
    "    total_error_rate = 0.0\n",
    "    n = 0  # Number of files processed\n",
    "    \n",
    "    # Shuffle the transcription list for each run if needed\n",
    "    if shuffle_transcript:  # Ensure shuffle_transcript is defined\n",
    "        random.shuffle(transcription_list)\n",
    "    \n",
    "    # Process in batches\n",
    "    #for i in range(0, len(transcription_list), batch_size):\n",
    "    with tqdm(range(0, len(transcription_list), batch_size), desc=f\"Processing Batch\") as t:\n",
    "        for i in t:\n",
    "        \n",
    "            batch_files = transcription_list[i:i + batch_size]\n",
    "            input_files = [os.path.join(path_to_pshr_raw_audio_data, f) for f, _ in batch_files]\n",
    "            \n",
    "            # Process batch transcriptions from ASR system\n",
    "            transcriptions = process_transcribe_batch(input_files, target_sr=16000, device=device, debug_printing=debug_printing)\n",
    "            \n",
    "            # Process each transcription in the batch\n",
    "            for j, (input_file, original_transcription) in enumerate(batch_files):\n",
    "                # Process transcription from ASR system\n",
    "                tekst = transcriptions[j]\n",
    "                \n",
    "                # Get original transcription from dictionary\n",
    "                original_transcription = ' '.join(audio_transcriptions[input_file])\n",
    "                \n",
    "                # Tokenize both transcriptions\n",
    "                original_tokens = tokenize_text(original_transcription)\n",
    "                asr_tokens = tokenize_text(tekst)\n",
    "            \n",
    "                # Normalize numbers in ASR tokens\n",
    "                normalized_asr_tokens = normalize_numbers(asr_tokens, norm_nums)\n",
    "            \n",
    "                # Align ASR tokens with original tokens based on character similarity\n",
    "                aligned_asr_tokens = align_tokens(original_tokens, normalized_asr_tokens, similarity_threshold=similarity_threshold)\n",
    "            \n",
    "                # Calculate error metrics\n",
    "                asr_deletion = len(original_tokens) - len(aligned_asr_tokens)\n",
    "                asr_addition = len(normalized_asr_tokens) - len(aligned_asr_tokens)\n",
    "                total_tokens += len(original_tokens)\n",
    "                error_rate = (asr_deletion + asr_addition) / len(original_tokens)\n",
    "            \n",
    "                # Accumulate metrics\n",
    "                total_asr_deletions += asr_deletion\n",
    "                total_asr_additions += asr_addition\n",
    "                total_error_rate += error_rate\n",
    "                n += 1\n",
    "            \n",
    "                # Print or use the error metric for each file as needed\n",
    "                if debug_printing == True:\n",
    "                    print(f\"Transcription {i + j + 1}: {tekst}\")\n",
    "                    print(f\"original_tokens: \\n{original_tokens}\\n\")\n",
    "                    print(f\"asr_tokens: \\n{asr_tokens}\\n\")\n",
    "                    print(f\"normalized_asr_tokens: \\n{normalized_asr_tokens}\\n\")\n",
    "                    print(f\"aligned_asr_tokens: \\n{aligned_asr_tokens}\\n\")\n",
    "                    print(f\"ASR Deletions: {asr_deletion}\")\n",
    "                    print(f\"ASR Additions: {asr_addition}\")\n",
    "                    print(f\"Total Tokens: {len(original_tokens)}\")\n",
    "                    print(f\"Error Rate (WER): {error_rate}\\n\")\n",
    "    \n",
    "    # Calculate average error metrics for this run\n",
    "    average_asr_deletions = total_asr_deletions / n\n",
    "    average_asr_additions = total_asr_additions / n\n",
    "    average_error_rate = total_error_rate / n\n",
    "    \n",
    "    # Print or use the average error metrics for the run as needed\n",
    "    print(f\"Average ASR Deletions: {average_asr_deletions}\")\n",
    "    print(f\"Average ASR Additions: {average_asr_additions}\")\n",
    "    print(f\"Average Error Rate (WER): {average_error_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# x scrap later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PRIMJER normalize_number POBOLJŠANJE WER\n",
    "15 lip 2024 21:14h\n",
    "\n",
    "original_tokens: \n",
    "['i', 'bit', 'ću', 'malo', 'neskroman', 'vi', 'mi', 'to', 'nećete', 'zamjeriti', 'mi', 'smo', 'pokazali', 'da', 'možemo', 'kao', 'manja', 'država', 'potaknuti', 'kolege', 'da', 'se', 'promjene', 'kriteriji', 'u', 'poglavlju', 'dvadeset', 'tri']\n",
    "\n",
    "asr_tokens: \n",
    "['i', 'bit', 'će', 'malo', 'neshroman', 'i', 'mi', 'to', 'nećete', 'zamjeriti', 'mi', 'smo', 'pokazali', 'da', 'možemo', 'kao', 'manja', 'država', 'potaknuti', 'kolege', 'da', 'se', 'promijene', 'kriteriji', 'u', 'poglavlju', '23']\n",
    "\n",
    "normalized_asr_tokens: \n",
    "['i', 'bit', 'će', 'malo', 'neshroman', 'i', 'mi', 'to', 'nećete', 'zamjeriti', 'mi', 'smo', 'pokazali', 'da', 'možemo', 'kao', 'manja', 'država', 'potaknuti', 'kolege', 'da', 'se', 'promijene', 'kriteriji', 'u', 'poglavlju', 'dvadeset', 'tri']\n",
    "\n",
    "aligned_asr_tokens: \n",
    "['i', 'bit', 'će', 'malo', 'neshroman', 'i', 'mi', 'to', 'nećete', 'zamjeriti', 'mi', 'smo', 'pokazali', 'da', 'možemo', 'kao', 'manja', 'država', 'potaknuti', 'kolege', 'da', 'se', 'promijene', 'kriteriji', 'u', 'poglavlju', 'dvadeset', 'tri']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-15T14:04:14.696510Z",
     "iopub.status.busy": "2024-06-15T14:04:14.696510Z",
     "iopub.status.idle": "2024-06-15T14:04:14.699823Z",
     "shell.execute_reply": "2024-06-15T14:04:14.699823Z",
     "shell.execute_reply.started": "2024-06-15T14:04:14.696510Z"
    }
   },
   "source": [
    "WORKING WITH v3.0 ## BOTH W and W/out LM WORKING\n",
    "CHECKED 15 lip 2024 17:45h\n",
    "\n",
    "VERSIONS OF PYTORCH\n",
    "\n",
    "Name: torch\n",
    "Version: 2.1.2\n",
    "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
    "Home-page: https://pytorch.org/\n",
    "Author: PyTorch Team\n",
    "Author-email: packages@pytorch.org\n",
    "License: BSD-3\n",
    "Location: C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\n",
    "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
    "Required-by: accelerate, optimum, sentence-transformers, torchaudio, torchvision\n",
    "WARNING: Skipping C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\pandas-2.1.4.dist-info due to invalid metadata entry 'name'\n",
    "Name: torchvision\n",
    "Version: 0.16.2\n",
    "Summary: image and video datasets and models for torch deep learning\n",
    "Home-page: https://github.com/pytorch/vision\n",
    "Author: PyTorch Core Team\n",
    "Author-email: soumith@pytorch.org\n",
    "License: BSD\n",
    "Location: C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\n",
    "Requires: numpy, pillow, requests, torch\n",
    "Required-by: sentence-transformers\n",
    "WARNING: Skipping C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\\pandas-2.1.4.dist-info due to invalid metadata entry 'name'\n",
    "Name: torchaudio\n",
    "Version: 2.3.1\n",
    "Summary: An audio package for PyTorch\n",
    "Home-page: https://github.com/pytorch/audio\n",
    "Author: Soumith Chintala, David Pollack, Sean Naren, Peter Goldsborough, Moto Hira, Caroline Chen, Jeff Hwang, Zhaoheng Ni, Xiaohui Zhang\n",
    "Author-email: soumith@pytorch.org\n",
    "License: \n",
    "Location: C:\\Users\\Public\\anaconda3\\envs\\PyPhon\\Lib\\site-packages\n",
    "Requires: torch\n",
    "Required-by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO3uz19EkBaZsrjq2rN6rV0",
   "provenance": [
    {
     "file_id": "1T9A_vTHdaB2oOuRCRRwTTAHs1379J-6l",
     "timestamp": 1718361648667
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
